<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>fundamentals of Natural Language Processing (NLP) in Python: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="00-introduction.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="01-preprocessing.html">2. Episode 1: From text to vectors</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="02-transformers.html">3. Episode 2: BERT and Transformers</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>

                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-00-introduction"><p>Content from <a href="00-introduction.html">Introduction</a></p>
<hr>
<p>Last updated on 2025-01-09 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/00-introduction.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is NLP?</li>
<li>What are real-world applications of NLP?</li>
<li>Which problems NLP solves best?</li>
<li>What is language from a NLP perspective?</li>
<li>How does NLP relates to Deep Learning and Machine Learning?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define Natural Language Processing</li>
<li>Detailing classic NLP tasks and applications in practice</li>
<li>Describe the theoretical perspectives that the field of NLP draws
upon, including linguistics (syntax, semantics, and pragmatics), Deep
Learning and Machine Learning</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-is-nlp">What is NLP?<a class="anchor" aria-label="anchor" href="#what-is-nlp"></a>
</h2>
<hr class="half-width">
<p>Natural language processing (NLP) is an area of research and
application that focuses on making natural (i.e., human) language
accessible to computers so that they can be used to perform useful
tasks. Research in NLP is highly interdisciplinary, drawing on concepts
from computer science, linguistics, logic, mathematics, psychology, etc.
In the past decade, NLP has evolved significantly with advances in
technology, especially in the field of deep learning, to the point that
it has become embedded in our daily lives.</p>
<p>Let’s start by looking at some popular applications you use in
everyday life that have some form of NLP component.</p>
<div id="nlp-in-the-real-world" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="nlp-in-the-real-world" class="callout-inner">
<h3 class="callout-title">NLP in the real world</h3>
<div class="callout-content">
<p>Name three to five tools/products that you use on a daily basis and
that you think leverage NLP techniques. To solve this exercise you can
get some help from the web.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>These are some of the most popular NLP-based products that we use on
a daily basis:</p>
<ul>
<li>Voice-based assistants (e.g., Alexa, Siri, Cortana)</li>
<li>Machine translation (e.g., Google translate, Amazon translate)</li>
<li>Search engines (e.g., Google, Bing, DuckDuckGo)</li>
<li>Keyboard autocompletion on smartphones</li>
<li>Spam filtering</li>
<li>Spell and grammar checking apps</li>
</ul>
</div>
</div>
</div>
</div>
<p>The exercise above tells us that a great deal of NLP techniques is
embedded in our daily life. Indeed NLP is an important component in a
wide range of software applications that we use in our daily lives.</p>
</section><section><h2 class="section-heading" id="core-applications">Core applications<a class="anchor" aria-label="anchor" href="#core-applications"></a>
</h2>
<hr class="half-width">
<ul>
<li><p>Email providers use NLP in several ways: automatically detect and
filter-out spam emails, classify important emails (e.g., Priority
inbox), recognise dates and events to automatically add them to your
calendar and suggesting you phrases while you’re typing</p></li>
<li><p>Voice-based assistants use NLP to recognise speech, interpret
requests (e.g., “Set alarm for 8 AM tomorrow”) and perform it
accurately, translate spoken language in real time and store individual
preferences and history to tailor responses based on the past activities
with the user</p></li>
<li><p>Search engines use NLP to interpret the meaning behind user
queries (e.g., “What’s the best restaurant near me?”), pull and
highlight key information directly from a webpage to answer your query
and personalise results based on user history</p></li>
</ul>
<div class="section level3">
<h3 id="other-types-of-applications">Other types of applications<a class="anchor" aria-label="anchor" href="#other-types-of-applications"></a>
</h3>
<ul>
<li><p>Customer care services use NLP to summarise and understand user
reviews to provide actionable insights to their companies</p></li>
<li><p>Spelling and grammar correction tools use NLP to highlight typos
or errors and suggest the most valid alternative</p></li>
<li><p>The <a href="https://historicalarchives.europarl.europa.eu/en/sites/historicalarchive/home/cultural-heritage-collections/news/ai-dashboard.html" class="external-link">Historical
Archives of the European Parliament</a> have used NLP to instantly
search, retrieve and understand decades of legislative documents and
parliamentary proceedings in multiple languages</p></li>
</ul>
</div>
</section><section><h2 class="section-heading" id="nlp-tasks">NLP tasks<a class="anchor" aria-label="anchor" href="#nlp-tasks"></a>
</h2>
<hr class="half-width">
<ul>
<li><p>Language modeling: Given a sequence of words, the model predicts
the next word. For example, in the sentence “The cat is on the _____”,
the model might predict “mat” based on the context. This task is useful
for building solutions that require speech and optical character
recognition (even handwriting), language translation and spelling
correction</p></li>
<li><p>Text classification: Given a set of items (e.g., emails), assign
a label (e.g., spam/not-spam). It is the task of assigning predefined
categories or labels to a given text. Text classification is extremely
popular in NLP applications, from spam filtering to movies ratings based
on reviews.</p></li>
<li><p>Information extraction: This is the task of extracting relevant
information from the text. “Eva Viviani, a Research Software Engineer at
the eScience Center, attended the 17th Conference of the European
Chapter of ACL on May 2nd, 2023”. Person: Eva Viviani, Job title: RSE,
Event: 17th Conference of the European Chapter of ACL, Date: May 2nd,
2023, etc.</p></li>
<li><p>Information retrieval: This is the task of finding relevant
information or documents from a large collection of unstructured data
based on user’s query, e.g., “What’s the best restaurant near
me?”.</p></li>
<li><p>Conversational agent (also known as ChatBot): Building a system
that interacts with a user via natural language, e.g., “What’s the
weather today, Siri?”. These agents are widely used to improve user
experience in customer service, personal assistance and many other
domains.</p></li>
<li><p>Topic modelling: Automatically identify abstract “topics” that
occur in a set of documents, where each topic is represented as a
cluster of words that frequently appear together. This task is used in a
variety of domains, from literature to bioinformatics as a common
text-mining tool.</p></li>
</ul></section><section><h2 class="section-heading" id="natural-vs-artificial-language">Natural vs Artificial Language<a class="anchor" aria-label="anchor" href="#natural-vs-artificial-language"></a>
</h2>
<hr class="half-width">
<p>Why does NLP have “natural” in its name? A <a href="https://en.wikipedia.org/wiki/Language" class="external-link">Language</a> is a
structured system of communication that consists of grammar and
vocabulary. Within this definition, in NLP we refer to human language as
<a href="https://en.wikipedia.org/wiki/Natural_language" class="external-link"><em>Natural</em></a>
to contrast it to <em>artificial</em> languages which are <a href="https://en.wikipedia.org/wiki/Formal_language" class="external-link">formal
languages</a>. The reason for this is that many experts believe that
<em>natural</em> languages have emerged <em>naturally</em> tens of
thousands of years ago and have evolved ever since. Formal languages, on
the other hand, are languages that have been engineered by humans and
have rigid and explicitly defined rules.</p>
<p>To understand this perspective, let’s consider for instance Python or
R. These are <a href="https://en.wikipedia.org/wiki/Programming_language" class="external-link">programming
languages</a> that have explicit, clear grammatical and syntactic rules.
This means that within the realm of those programming languages, there
is no room for ambiguity, otherwise your code would allow for different
behaviours depending on the situation, or the machine. This is not the
case for human languages.</p>
<div class="section level3">
<h3 id="ambiguity">Ambiguity<a class="anchor" aria-label="anchor" href="#ambiguity"></a>
</h3>
<p>Natural language is highly creative and often ambiguous, among many
other complex traits. A sentence of the type “I saw a bat” may mean many
things depending on who is hearing/saying it, where and when it is
pronounced. The disambiguation of meaning is usually a by-product of the
context in which sentences are pronounced and the historic accumulation
of interactions which are transmitted across generations (think for
instance to idioms – these are usually meaningless phrases that acquire
meaning only if situated within their historical and societal context).
These characteristics make NLP a particularly challenging field to work
in.</p>
<p>We cannot expect a machine to process human language and simply
understand it as it is. We need a systematic, scientific approach to
deal with it. It’s within this premise that the field of NLP is born,
primarily interested in converting the building blocks of human/natural
language into something that a machine can understand. We’ll see what
does this mean in the next episode.</p>
<p>The image below shows you the building blocks of language and a few
NLP applications that leverage this type of information.</p>
<div class="float">
<embed src="fig/intro.pdf"></embed><div class="figcaption">Diagram showing building blocks of
language</div>
</div>
<p>Each building block of human language carries a large amount of
information, which we process quickly and effortlessly. Some of this
information is still being studied by scientists because it’s unclear
how to measure it, whether the human brain uses it at all to aid
understanding, and, if so, to what extent. A lot of research effort is
spent on this problem in academia, and it’s important to keep in mind
that we are still far from solving it.</p>
<p>How do we make language then understandable for machines? How do we
expose and exploit the statistical information within the human
language? The field of NLP focuses exactly on these challenges. The
ultimate goal is to make this information available to computers, so
that they can use it to understand language as closely as possible to
the way we (humans) do.</p>
</div>
<div class="section level3">
<h3 id="discreteness">Discreteness<a class="anchor" aria-label="anchor" href="#discreteness"></a>
</h3>
<p>NLP is a subfield of Artificial Intelligence that intersects with <a href="https://carpentries-incubator.github.io/deep-learning-intro/1-introduction.html" class="external-link">Deep
Learning</a> and more broadly with <a href="https://en.wikipedia.org/wiki/Machine_learning" class="external-link">Machine
Learning</a>. There are many concepts in NLP that indeed draw upon those
fields. For instance the task of categorising text as positive or
negative, is a classification <a href="https://carpentries-incubator.github.io/deep-learning-intro/2-keras.html#formulateoutline-the-problem-penguin-classification" class="external-link">problem</a>
that has been formulated and solved also in the Deep Learning realm.
What’s the difference then between classifying which species a penguin
belongs to (based on their pictures) and understand the difference
between “cat” and “sat”?</p>
<p>If you take an image of a penguin and change a pixel it will still be
recognised as the same penguin as before. This tiny change has resulted
in a small change that did not affect the whole picture. If you change
one letter of a word, however, as in cat vs sat, then even if the
difference for the computer is a single bit, the two things in the human
language are two separate, <em>discrete</em> concepts. They just happen
(for historical reasons or just by chance) to have similar
spellings.</p>
<p>The reason why NLP is a distinct field is that, unlike images and
sounds (which are typically handled in Deep Learning and are continuous
data), words are discrete units. This characteristic of human language
demands a completely different approach because, while computers excel
at processing continuous variables, they struggle with the discrete
nature of language. In the next episode, we’ll explore how a solution to
this challenge has only recently been developed.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>NLP is embedded in numerous daily-use products</li>
<li>Key tasks include language modeling, text classification,
information extraction, information retrieval, conversational agents,
and topic modeling, each supporting various real-world
applications.</li>
<li>NLP is a subfield of Artificial Intelligence (AI) that deals with
approaches to process, understand and generate natural language</li>
<li>Deep learning has significantly advanced NLP, but the challenge
remains in processing the discrete and ambiguous nature of language</li>
<li>The ultimate goal of NLP is to enable machines to understand and
process language as humans do, but challenges in measuring and
interpreting linguistic information still exist.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-01-preprocessing"><p>Content from <a href="01-preprocessing.html">Episode 1: From text to vectors</a></p>
<hr>
<p>Last updated on 2025-01-09 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-preprocessing.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Why do we need to prepare a text for training?</li>
<li>How do I prepare a text to be used as input to a model?</li>
<li>What different types of pre processing steps are there?</li>
<li>How do I train a neural network to extract word embeddings?</li>
<li>What properties word embeddings have?</li>
<li>What is a word2vec model?</li>
<li>How do we train a word2vec model?</li>
<li>How do I get insights regarding my text, based on the word
embeddings?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul>
<li>Implement a full preprocessing pipeline on a text</li>
<li>Use Word2Vec to train a model</li>
<li>Inspect word embeddings</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>In this episode, we’ll train a neural network to obtain word
embeddings. We will only briefly touch upon the concepts of
<code>preprocessing</code> and <code>word embedding</code> with
Word2vec.</p>
<p>The idea is to get you over a practical example first, without diving
into the technical or mathematical intricacies of neural networks and
word embeddings. The goal for this episode, in fact, is for you to get
an intuition of how computers represent language. This is key to
understand how NLP applications work and what are their limits.</p>
<p>In the later episodes we will build upon this knowledge to go deeper
into all of these concepts and see how NLP tools have evolved more
complex language representations.</p>
<p>In this episode, we will build a workflow following these steps:</p>
<ol style="list-style-type: decimal">
<li>Formulate the problem</li>
<li>Download the input data</li>
<li>Prepare data to be ingested by the model (i.e. preprocessing
step)</li>
<li>Train the model</li>
<li>Load the embeddings and inspect them</li>
</ol>
<p>Note that for step 5 we will cover only briefly the code to train
your own model, but then we will load the output of already pretrained
models. That is because training requires a large amount of data and
considerable computing resources/time which are not suitable for a local
laptop/computer.</p>
</section><section><h2 class="section-heading" id="formulate-the-problem">1. Formulate the problem<a class="anchor" aria-label="anchor" href="#formulate-the-problem"></a>
</h2>
<hr class="half-width">
<p>In this episode we will be using Dutch newspaper texts to train a
Word2Vec model to investigate the notion of <em>semantic shift</em>.</p>
</section><section><h2 class="section-heading" id="semantic-shift">Semantic shift<a class="anchor" aria-label="anchor" href="#semantic-shift"></a>
</h2>
<hr class="half-width">
<p>Semantic shift, as it is used here, refers to a pair of meanings A
and B which are linked by some relation. Either diachronically (e.g.,
Latin <em>caput</em> “head” and Italian <em>capo</em> “chief”) or
synchronically, e.g. as two meanings that co-exist in a word
simoultaneously (English “head”, as in “I have covered my head with a
hat” and as in “I am the head of the department”). <strong>Can we detect
a semantic shift?</strong> – We’ll tackle this phenomenon in this
episode.</p>
<p>Newspapers make an interesting dataset for investigating this
phenomenon, as they contain information about current events and the
language it uses is clear and reflective of its time. We will
specifically look at the evolution of specific words in Dutch across a
period of time from 1950 to 1990. In order to do that, we need to train
a model to extract the meaning of every single word and track in which
context they occur, over decades.</p>
<div id="goal" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="goal" class="callout-inner">
<h3 class="callout-title">Goal</h3>
<div class="callout-content">
<p>The goal is to analyze the semantic shift of specific Dutch words
from 1950 to 1989 using newspapers as a dataset.</p>
</div>
</div>
</div>
<p>Conceptually, the task of discovery of semantic shifts in our
newspaper data can be formulated as follows:</p>
<p>Given newspaper corpora [C1, C2, C3, …] containing texts created in
time periods from 1950s to 1980s, considered as four decades [1: 50-60;
2: 60-70; 3: 70-80; 4: 80-90], the task is to detect how some words have
shifted in meaning across those decades.</p>
<p>As a test-bed, we’re going to focus on three words:
<code>mobiel</code>, <code>televisie</code> and <code>ijzeren</code>.
These words exemplify very well the notion of semantic evolution /
semantic shift, as their meaning has gained new nuances due to social,
technological, political and economic changes occurred in those key
years.</p>
<p>We’re going to use a model to solve this task. We’re going to see
which one and how in a moment.</p>
<p>Our dataset is provided by Delpher (developed by the <a href="https://www.kb.nl/" class="external-link">KB - the National Library of the
Netherlands</a>) which contains digitalised historic Dutch newspapers,
books, and magazines. This online newspaper collection covers data
spanning from 1618 up to 1995 and of many local, national and
international publishers.</p>
<p>We will load only a page to go step-by-step through what it takes to
train a model. This makes it easier to know what’s going on. However, in
practice, when to successfully train a model you need larger quantities
of data to allow the model to get more precise and accurate
representations. In those cases you will simply condense each of the
steps we cover next into one code, to do all these steps at once.</p>
<div id="dataset-size-in-training" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="dataset-size-in-training" class="callout-inner">
<h3 class="callout-title">Dataset size in training</h3>
<div class="callout-content">
<p>To obtain high-quality embeddings, the size/length of your training
dataset plays a crucial role. Generally <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" class="external-link">tens of
thousands of documents</a> are considered a reasonable amount of data
for decent results.</p>
<p>Is there however a strict minimum? Not really. Things to keep in mind
is that <code>vocabulary size</code>, <code>document length</code> and
<code>desired vector size</code> interacts with each other. The higher
the dimensional vectors (e.g. 200-300 dimensions) the more data is
required, and of high quality, i.e. that allows the learning of words in
a variety of contexts.</p>
<p>While word2vec models typically perform better with large datasets
containing millions of words, using a single page is sufficient for
demonstration and learning purposes. This smaller dataset allows us to
train the model quickly and understand how word2vec works without the
need for extensive computational resources.</p>
</div>
</div>
</div>
<p>For the purpose of this episode and to make training easy on our
laptop, we’ll train our word2vec model using <strong>just one
page</strong>. Subsequently, we’ll load pre-trained models for tackling
our task.</p>
<div id="exploring-delpher" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exploring-delpher" class="callout-inner">
<h3 class="callout-title">Exploring Delpher</h3>
<div class="callout-content">
<p>Before we move further with our problem, take your time to explore
Delpher more in detail. Go to <a href="https://www.delpher.nl/" class="external-link">Delpher</a> and pick a newspaper of a
particular date. Did you find anything in the newspaper that is
interesting or didn’t know yet? For example about your living area,
sports club, or an historic event?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Few examples. </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li><p>The 20th of July 1969 marks an important event. The First Moon
landing! Look at what the <a href="https://resolver.kb.nl/resolve?urn=KBPERS01:003319021:mpeg21:pdf" class="external-link">Tubantia
newspaper</a> had to say about it only four days afterwards.</p></li>
<li><p>The Cuban Missile Crisis, also known as the October Crisis in
Cuba, or the Caribbean Crisis, was a 13-day confrontation between the
governments of the United States and the Soviet Union, when American
deployments of nuclear missiles in Italy and Turkey were matched by
Soviet deployments of nuclear missiles in Cuba. The crisis lasted from
16 to 28 October 1962. See what de Volkskrant published on the <a href="https://resolver.kb.nl/resolve?urn=ABCDDD:010876534:mpeg21:pdf" class="external-link">24th
of October, 1962</a>. Can you see what they have organised in Den Haag
related to this event?</p></li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="download-the-data">2. Download the data<a class="anchor" aria-label="anchor" href="#download-the-data"></a>
</h2>
<hr class="half-width">
<p>We download a page from the journal <a href="https://www.delpher.nl/nl/kranten/view?coll=ddd&amp;query=&amp;cql%5B%5D=%28date+_gte_+%2220-07-1969%22%29&amp;redirect=true&amp;sortfield=date&amp;resultscoll=dddtitel&amp;identifier=KBPERS01:002846018:mpeg21&amp;rowid=3" class="external-link">Algemeen
Dagblad</a> of July 21, 1969 as <code>txt</code> and save it as
<code>ad.txt</code>. We then load this file and store it in a variable
called <code>corpus</code>.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>path <span class="op">=</span> <span class="st">"episodes/data/ad.txt"</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path) <span class="im">as</span> myfile:</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>    corpus <span class="op">=</span> myfile.read()</span></code></pre>
</div>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>The <code>txt</code> file provides the text without formatting and
images, and is the product of a technique called Optical Character
Recognition (OCR). This is a technique in which text from an image is
converted into text, and it’s a necessary step for any scanned image to
obtain plain text. Luckily for us, Delpher has already done this step
for us so that we can directly use the txt. However, take into
consideration that if you start from an image that contains text, you
may need an additional preprocessing step.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="inspect-the-data">Inspect the data<a class="anchor" aria-label="anchor" href="#inspect-the-data"></a>
</h2>
<hr class="half-width">
<p>We inspect the first line of the imported text:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>corpus[:<span class="dv">100</span>]</span></code></pre>
</div>
<p><code>'MENS OP MAAN\n„De Eagle is geland” Reisduur: 102 uur, Uitstappen binnen 20 iuli, 21.17 uur 45 min. en'</code></p>
<p>We can see that although the OCR applied to the original image has
given a pretty good result, there are mistakes in the recognized text.
For example, on the first line the word <code>juli</code> (july) has
misinterpreted as <code>iuli</code>.</p>
<p>Note also the size of the text:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="bu">len</span>(corpus)</span></code></pre>
</div>
<p>There are <code>12354</code> characters inside the corpus. Note also
the type of file:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">type</span>(corpus)</span></code></pre>
</div>
<p>Python tells us that <code>corpus</code> is a <code>str</code>,
i.e. a string. This means that every single character in the text (even
blank spaces) is a unit for our computer. However, what’s really
important for us is that the machine gets the meaning of the
<strong>words</strong> contained in the text. That is, that is able to
understand which characters belong together to form a word, and what
instead is something else: Punctuation, conjunctions, articles, or
prepositions.</p>
<p>How do we teach our machine to <em>segment</em> the text and keep
only the relevant words? This is where <code>data preprocessing</code>
comes into play. It prepares the text for efficient processing by the
model, allowing it to focus on the important parts of the text that
contribute to understanding its meaning.</p>
</section><section><h2 class="section-heading" id="prepare-data-to-be-ingested-by-the-model-preprocessing">3. Prepare data to be ingested by the model (preprocessing)<a class="anchor" aria-label="anchor" href="#prepare-data-to-be-ingested-by-the-model-preprocessing"></a>
</h2>
<hr class="half-width">
<p>NLP models work by learning the statistical regularities within the
constituent parts of the language (i.e, letters, digits, words and
sentences) in a text. However, text contains also other type of
information that humans find useful to convey meaning. To signal pauses,
give emphasis and convey tone, for instance, we use punctuation.
Articles, conjunctions and prepositions also alter the meaning of a
sentence. The machine does not know the difference among all of these
linguistic units, as it treats them all as equal. Also, the decision to
remove or retain these parts of text is quite crucial for training our
model, as it affects the quality of generated word vectors.</p>
<p>Examples of preprocessing steps are:</p>
<ul>
<li>Cleaning the text: remove symbols/special characters, or other
things that “sneaked” into the text while loading the original
version.</li>
<li>Lowercasing</li>
<li>Removing punctuation</li>
<li>Stop word removal, where you remove prepositions, conjuctions and
articles</li>
<li>Tokenization: this means segmenting the text by retaining groups of
characters. These groups are referred to as <code>tokens</code> and
their size can vary from entire words to lemmas, or subword components
(e.g. morphemes)</li>
<li>Part of speech tagging: the process of labelling the grammatical
role of a word, e.g. nouns and verbs.</li>
</ul>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<ul>
<li><p>Preprocessing approaches affect significantly the quality of the
training when working with word embeddings. For example, [Rahimi &amp;
Homayounpour (2022)] (<a href="https://link.springer.com/article/10.1007/s10579-022-09620-5" class="external-link uri">https://link.springer.com/article/10.1007/s10579-022-09620-5</a>)
demonstrated that for text classification and sentiment analysis, the
removal of punctuation and stopwords leads to higher
performance.</p></li>
<li><p>You do not always need to do all the preprocessing steps, and
which ones you should do depends on what you want to do. For example, if
you want to extract entities from the text using named entity
recognition, you explicitly do not want to lowercase the text, as
capitals are a component in the identification process.</p></li>
<li><p>Preprocessing can be very diffent for different languages. This
is both in terms of which steps to apply, but also which methods to use
for a specific step.</p></li>
</ul>
</div>
</div>
</div>
<p>Let’s apply a number of preprocessing steps to extract a list of
words from the newspaper page.</p>
<div class="section level4">
<h4 id="cleaning-the-text">1. Cleaning the text<a class="anchor" aria-label="anchor" href="#cleaning-the-text"></a>
</h4>
<p>We start by importing the <code>spaCy</code> library that will help
us go through the preprocessing steps. SpaCy is a popular open-source
library for NLP in Python and it works with pre-trained languages models
that we can load and use to process and analyse the text
efficiently.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> spacy</span></code></pre>
</div>
<p>We need to install <code>en_core_web_sm</code> because the text we’re
dealing with it’s in Dutch This is a small pre-trained language <a href="https://spacy.io/models/nl/" class="external-link">model from Spacy</a> containing
essential components like vocabulary, syntax, and entities specifically
for the Dutch language.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>python <span class="op">-</span>m spacy download nl_core_news_sm</span></code></pre>
</div>
<p>We can then load the model into the pipeline function. This function
connects the pretrained model to various preprocessing steps, including
the tokenisation.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(corpus)</span></code></pre>
</div>
<p>Next, we’ll eliminate the triple dashes that separate different news
articles, as well as the vertical bars used to divide some columns.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># filter out triple dashes and vertical bars</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>filtered_tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.text <span class="op">!=</span> <span class="st">"---"</span> <span class="kw">and</span> token.text <span class="op">!=</span> <span class="st">"|"</span>]</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co"># join units back into a cleaned string</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>corpus_clean <span class="op">=</span> <span class="st">' '</span>.join(filtered_tokens)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="bu">print</span>(corpus_clean[:<span class="dv">100</span>])</span></code></pre>
</div>
<p><code>MENS OP MAAN „ De Eagle is geland ” Reisduur : 102 uur , Uitstappen binnen 20 iuli , 21.17 uur 45</code></p>
</div>
<div class="section level4">
<h4 id="lowercasing">2. Lowercasing<a class="anchor" aria-label="anchor" href="#lowercasing"></a>
</h4>
<p>Our next step is to lowercase the text. Our goal here is to generate
a list of unique words from the text, so in order to not have words
twice in the list - once normal and once capitalised when it is at the
start of a sentence for example - we can lowercase the full text.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>corpus_lower <span class="op">=</span> corpus_clean.lower()</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="bu">print</span>(corpus_lower)</span></code></pre>
</div>
<p><code>mens op maan \n „ de eagle is geland ” reisduur : 102 uur , uitstappen binnen 20 iuli , 21.17 uur 45 [...]</code></p>
<div id="callout5" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>It is important to keep in mind that in doing this, some information
is lost. As mentioned before, models that are trained to identify named
entities use information on capitalisation. As another example, there
are a lot of names and surnames that carry meaning. “Bakker” is a common
Dutch surname, but is also a noun (baker). In lowercasing the text you
loose the distinction between the two.</p>
</div>
</div>
</div>
<p>Next we move to tokenise our text.</p>
</div>
<div class="section level4">
<h4 id="tokenisation">4. Tokenisation<a class="anchor" aria-label="anchor" href="#tokenisation"></a>
</h4>
<p>Tokenisation is essential in NLP, as it helps to create structure
from raw text. It involves the segmentation of the text into smaller
units referred as <code>tokens</code>. Tokens can be sentences
(e.g. <code>'the happy cat'</code>), words
(<code>'the', 'happy', 'cat'</code>), subwords
(<code>'un', 'happiness'</code>) or characters
(<code>'c','a', 't'</code>). The choice of tokens depends by the
requirement of the model used for training, and the text. This step is
carried out by a pre-trained model (called tokeniser) that has been
fine-tuned for the target language. In our case, this is
<code>en_core_web_sm</code> loaded before.</p>
<div id="callout6" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>A good word tokeniser for example, does not simply break up a text
based on spaces and punctuation, but it should be able to
distinguish:</p>
<ul>
<li>abbreviations that include points (e.g.: <em>e.g.</em>)</li>
<li>times (<em>11:15</em>) and dates written in various formats
(<em>01/01/2024</em> or <em>01-01-2024</em>)</li>
<li>word contractions such as <em>don’t</em>, these should be split into
<em>do</em> and <em>n’t</em>
</li>
<li>URLs</li>
</ul>
<p>Many older tokenisers are rule-based, meaning that they iterate over
a number of predefined rules to split the text into tokens, which is
useful for splitting text into word tokens for example. Modern large
language models use subword tokenisation, which are more flexible.</p>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>spacy_corpus <span class="op">=</span> nlp(corpus_clean)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co"># Get the tokens from the pipeline</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> spacy_corpus]</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>tokens[:<span class="dv">10</span>]</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', '\n ', '„', 'de', 'eagle', 'is', 'geland', '”']</code></p>
<p>As one can see the tokeniser has split each word in a token, however
it has considered also blank spaces <code>\n</code> and also
punctuation.</p>
</div>
<div class="section level4">
<h4 id="remove-punctuation">5. Remove punctuation<a class="anchor" aria-label="anchor" href="#remove-punctuation"></a>
</h4>
<p>The next step we will apply is to remove punctuation. We are
interested in training our model to learn the meaning of the words. This
task is highly influenced by the state of our text and punctuation would
decrease the quality of the learning as it would add spurious
information. We’ll see how the learning process works later in the
episode.</p>
<p>The punctuation symbols are defined in:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>string.punctuation</span></code></pre>
</div>
<p>We can loop over these symbols to remove them from the text:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># remove punctuation from set</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> string.punctuation]</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co"># remove also blank spaces</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens_no_punct <span class="cf">if</span> token.strip() <span class="op">!=</span> <span class="st">''</span>]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="bu">print</span>(tokens_no_punct[:<span class="dv">10</span>])</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', 'de', 'eagle', 'is', 'geland', 'reisduur', '102', 'uur']</code></p>
</div>
<div class="section level4">
<h4 id="visualise-the-tokens">Visualise the tokens<a class="anchor" aria-label="anchor" href="#visualise-the-tokens"></a>
</h4>
<p>This was the end of our preprocessing step. Let’s look at what tokens
we have extracted and how frequently they occur in the text.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="co"># count the frequency of occurrence of each token</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>token_counts <span class="op">=</span> Counter(tokens_no_punct)</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a><span class="co"># get the top n most common tokens (otherwise the plot would be too crowded) and their relative frequencies</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>most_common <span class="op">=</span> token_counts.most_common(<span class="dv">100</span>)</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>tokens <span class="op">=</span> [item[<span class="dv">0</span>] <span class="cf">for</span> item <span class="kw">in</span> most_common]</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>frequencies <span class="op">=</span> [item[<span class="dv">1</span>] <span class="cf">for</span> item <span class="kw">in</span> most_common]</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>plt.bar(tokens, frequencies)</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>plt.xlabel(<span class="st">'Tokens'</span>)</span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a>plt.title(<span class="st">'Token Frequencies'</span>)</span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<p>As one can see, words in the text have a very specific <a href="https://link.springer.com/article/10.3758/s13423-014-0585-6" class="external-link">skewed
distribution</a>, such that there are few very high-frequency words that
account for most of the tokens in text (e.g., articles, conjunctions)
and many low frequency words.</p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Discuss with each other:</p>
<ul>
<li>For which NLP tasks can punctuation removal be applied?</li>
<li>For which tasks is punctuation relevant and should punctuation not
be removed?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ul>
<li>Laura left the solution here missing – could this be considered an
exercise without a solution?</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="stop-word-removal">6. Stop word removal<a class="anchor" aria-label="anchor" href="#stop-word-removal"></a>
</h4>
<p>For some NLP tasks only the important words in the text are needed. A
text however often contains many <code>stop words</code>: common words
such as <code>de</code>, <code>het</code>, <code>een</code> that add
little meaningful content compared to nouns and words. In those cases,
it is best to remove stop words from your corpus to reduce the number of
words to process.</p>
<div id="tasks-where-stop-word-removal-is-useful" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="tasks-where-stop-word-removal-is-useful" class="callout-inner">
<h3 class="callout-title">Tasks where stop word removal is useful</h3>
<div class="callout-content">
<p>NLP tasks for which stop word removal can be applied are for example
<code>text classification</code> or <code>topic modelling</code>. When
clustering words into topics, stop words are irrelevant. Having fewer
and more relevant words gives better results. For other tasks, such as
<code>text generation</code> or <code>question answering</code>, the
full structure and context are important, so stop words should
<em>not</em> be removed. This is also the case for
<code>named entity recognition</code>, since named entities can contain
stop words themselves.</p>
</div>
</div>
</div>
<p>The Dutch spaCy model contains a list of stop words in the Dutch
language.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>stopwords <span class="op">=</span> nlp.Defaults.stop_words</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(stopwords)[:<span class="dv">20</span>])</span></code></pre>
</div>
<p><code>['bijvoorbeeld', 'ikzelf', 'anderzijds', 'toch', 'jouwe', 'omtrent', 'geleden', 'een', 'met', 'voorts', 'pas', 'zal', 'meer', 'maar', 'wier', 'hen', 'hare', 'vervolgens', 'klaar', 'worden']</code></p>
<p>We proceed to remove it:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="co"># remove stopwords</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>tokens_no_stopwords <span class="op">=</span> tokens_no_punct</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a><span class="cf">for</span> stopword <span class="kw">in</span> stopwords:</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    tokens_no_stopwords <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens_no_stopwords <span class="cf">if</span> token <span class="op">!=</span> stopword]</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a><span class="bu">print</span>(tokens_no_stopwords[:<span class="dv">20</span>])</span></code></pre>
</div>
<p><code>['mens', 'maan', 'eagle', 'geland', 'reisduur', '102', 'uur', 'uitstappen', '20', 'iuli', '21.17', 'uur', '45', 'min.', '40', 'sec.', 'vijf', 'uur', 'landing', 'armstrong']</code></p>
</div>
<div class="section level4">
<h4 id="visualise-tokens-into-a-word-cloud">Visualise tokens into a word cloud<a class="anchor" aria-label="anchor" href="#visualise-tokens-into-a-word-cloud"></a>
</h4>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">from</span> wordcloud <span class="im">import</span> WordCloud</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>wordcloud <span class="op">=</span> WordCloud().generate(<span class="st">' '</span>.join(tokens_no_stopwords))</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>plt.imshow(wordcloud, interpolation<span class="op">=</span><span class="st">'bilinear'</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Preprocessing involves a number of steps that one can apply to their
text to prepare it for further processing.</li>
<li>Preprocessing is important because it can improve your results</li>
<li>You do not always need to do all preprocessing steps. It depends on
the task at hand which preprocessing steps are important.</li>
</ul>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="tracing-semantic-shifts-with-word-embeddings">Tracing semantic shifts with word embeddings<a class="anchor" aria-label="anchor" href="#tracing-semantic-shifts-with-word-embeddings"></a>
</h2>
<hr class="half-width">
<p>Now we will train a model to to detect how the meaning of
<code>ijzeren</code>, <code>televisie</code> and <code>mobiel</code>
have shifted over the years, from the 50s to the 80s. This model will
return us <code>distributional word representations</code>, also known
as <code>embeddings</code>.</p>
<p>A number of publications (e.g., <a href="https://www.jair.org/index.php/jair/article/view/10640" class="external-link">Turney et
al., 2010</a>; <a href="https://aclanthology.org/P14-1023.pdf" class="external-link">Baroni et
al., 2014</a>) have showed that embeddings provide an efficient way to
track how meanings of words change across years. Let’s see what are
those and how they manage to do that.</p>
</section><section><h2 class="section-heading" id="what-are-word-embeddings">What are word embeddings?<a class="anchor" aria-label="anchor" href="#what-are-word-embeddings"></a>
</h2>
<hr class="half-width">
<p>A Word Embedding is a word representation type that maps words in a
numerical manner (i.e., into vectors) in a multidimensional space,
capturing their meaning based on characteristics or context. Since
similar words occur in similar contexts, or have same characteristics,
the system naturally learns to assign similar vectors to similar
words.</p>
<p>Let’s illustrate this concept using animals. This example will show
us an intuitive way of representing things into vectors.</p>
<p>Suppose we want to represent a <code>cat</code> using measurable
characteristics:</p>
<ul>
<li>Furriness: Let’s assign a score of 70 to a cat</li>
<li>Number of legs: A cat has 4 legs</li>
</ul>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>cat <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">4</span>]])</span></code></pre>
</div>
<p>So the vector representation of a cat becomes:
<code>[70 (furriness), 4 (legs)]</code></p>
<figure><img src="fig/emb3.png" class="figure mx-auto d-block"><div class="figcaption">Embedding of a cat - We have described it along
two dimensions: furriness and number of legs</div>
</figure><p>This vector doesn’t fully describe a cat but provides a basis for
comparison with other animals.</p>
<p>Let’s add vectors for a dog and a caterpillar:</p>
<ul>
<li>Dog: [56, 4]</li>
<li>Caterpillar: [70, 100]</li>
</ul>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>dog <span class="op">=</span> np.array([[<span class="dv">56</span>, <span class="dv">4</span>]])</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">100</span>]])</span></code></pre>
</div>
<figure><img src="fig/emb5.png" class="figure mx-auto d-block"><div class="figcaption">Embeddings of a cat and a dog and a
caterpillar</div>
</figure><p>To determine which animal is more similar to a cat, we use
<code>cosine similarity</code>, which measures the cosine of the angle
between two vectors.</p>
<div id="callout8" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p><a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> ranges between [<code>-1</code> and <code>1</code>]. It
is the cosine of the angle between two vectors, divided by the product
of their length. It is a useful metric to measure how similar two
vectors are likely to be.</p>
<figure><img src="fig/emb12.png" alt="" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>similarity_cat_dog <span class="op">=</span> cosine_similarity(cat, dog)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>similarity_cat_caterpillar <span class="op">=</span> cosine_similarity(cat, caterpillar)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and dog: </span><span class="sc">{</span>similarity_cat_dog<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and caterpillar: </span><span class="sc">{</span>similarity_cat_caterpillar<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> dog: <span class="fl">0.9998987965747193</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> caterpillar: <span class="fl">0.6192653797321375</span></span></code></pre>
</div>
<p>The higher similarity score between the cat and the dog indicates
they are more similar based on these characteristics. Adding more
characteristics can enrich our vectors, detecting more semantic
nuances.</p>
<figure><img src="fig/emb6.png" class="figure mx-auto d-block"><div class="figcaption">Embeddings of a cat and a dog and a caterpillar
- We can describe these animals in many dimensions!</div>
</figure><div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ul>
<li>Add one of two other dimensions. What characteristics could they
map?</li>
<li>Add another animal and map their dimensions</li>
<li>Compute again the cosine similarity among those animals and find the
couple that is the least similar and the most similar</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>Add one of two other dimensions</li>
</ol>
<p>We could add the dimension of “velocity” or “speed” that goes from 0
to 100 meters/second.</p>
<ul>
<li>Caterpillar: 0.001 m/s</li>
<li>Cat: 1.5 m/s</li>
<li>Dog: 2.5 m/s</li>
</ul>
<p>(just as an example, actual speeds may vary)</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>]])</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>]])</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>]])</span></code></pre>
</div>
<p>Another dimension could be weight in Kg:</p>
<ul>
<li>Caterpillar: .05 Kg</li>
<li>Cat: 4 Kg</li>
<li>Dog: 15 Kg</li>
</ul>
<p>(just as an example, actual weight may vary)</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>, <span class="dv">4</span>]])</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>, <span class="dv">15</span>]])</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>, <span class="fl">.05</span>]])</span></code></pre>
</div>
<p>Then the cosine similarity would be:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>cosine_similarity(cat, caterpillar)</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>cosine_similarity(cat, dog)</span></code></pre>
</div>
<p>Output:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>array([[<span class="fl">0.61814254</span>]])</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>array([[<span class="fl">0.97893809</span>]])</span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Add another animal and map their dimensions</li>
</ol>
<p>Another animal that we could add is the Tarantula!</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>, <span class="dv">4</span>]])</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>, <span class="dv">15</span>]])</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>, <span class="fl">.05</span>]])</span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>tarantula <span class="op">=</span> np.asarray([[<span class="dv">80</span>, <span class="dv">6</span>, <span class="fl">.1</span>, <span class="fl">.3</span>]])</span></code></pre>
</div>
<ol start="3" style="list-style-type: decimal">
<li>Compute again the cosine similarity among those animals - find out
the most and least similar couple</li>
</ol>
<p>Given the values above, the least similar couple is the dog and the
caterpillar, whose cosine similarity is
<code>array([[0.60855407]])</code>.</p>
<p>The most similar couple is the cat and the tarantula:
<code>array([[0.99822302]])</code></p>
</div>
</div>
</div>
</div>
<p>By representing words as vectors with multiple dimensions, we capture
more nuances of their meanings or characteristics.</p>
<div id="keypoints2" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>We can represent text as vectors of numbers (which makes it
interpretable for machines)</li>
<li>The most efficient and useful way is to use word embeddings</li>
<li>We can easily compute how words are similar to each other with the
cosine similarity</li>
</ul>
</div>
</div>
</div>
<p>When semantic change occurs, words in their context <em>also</em>
change. We can trace how a word evolves semantically over time through
comparison of that word with other similar words. The idea is that the
most similar words are not always fixed in each different year, if a
word acquires a new meaning.</p>
<div id="you-shall-know-a-word-by-the-company-it-keeps---j.-r.-firth-1957" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="you-shall-know-a-word-by-the-company-it-keeps---j.-r.-firth-1957" class="callout-inner">
<h3 class="callout-title">
<strong>You shall know a word by the company
it keeps</strong> - J. R. Firth, 1957</h3>
<div class="callout-content">
<ul>
<li><p>A word which holds the same meaning across time has stable
contexts and similar words</p></li>
<li><p>A word that instead shifts meaning will be reflected by different
contexts and words</p></li>
</ul>
<p>So the changes of most similar words reflect the semantic change.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="train-the-word2vec-model">4. Train the Word2Vec model<a class="anchor" aria-label="anchor" href="#train-the-word2vec-model"></a>
</h2>
<hr class="half-width">
<p>Now we will train a two-layer neural network to transform our tokens
into word embeddings. We will be using the library <code>gensim</code>
and the model we will be using is called <code>Word2Vec</code>,
developed by Tomas Mikolov et al. in 2013.</p>
<p>Import the necessary libraries:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a><span class="co"># import logging to monitor training</span></span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a><span class="co"># set up logging</span></span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a>logging.basicConfig(<span class="bu">format</span><span class="op">=</span><span class="st">'</span><span class="sc">%(asctime)s</span><span class="st"> : </span><span class="sc">%(levelname)s</span><span class="st"> : </span><span class="sc">%(message)s</span><span class="st">'</span>, level<span class="op">=</span>logging.INFO)</span></code></pre>
</div>
<p>There are two main architectures for training Word2Vec:</p>
<ul>
<li>Continuous Bag-of-Words (CBOW): Predicts a target word based on its
surrounding context words.</li>
<li>Continuous Skip-Gram: Predicts surrounding context words given a
target word.</li>
</ul>
<figure><img src="fig/emb13.png" class="figure mx-auto d-block"><div class="figcaption">Schematic representations of the different
prediction tasks that CBOW and Skip-gram try to solve</div>
</figure><div id="callout10" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>CBOW is faster to train, while Skip-Gram is more effective for
infrequent words. Increasing context size improves embeddings but
increases training time.</p>
</div>
</div>
</div>
<p>We will be using CBOW. We are interested in having vectors with 300
dimensions and a context size of 5 surrounding words. We include all
words present in the corpora, regardless of their frequency of
occurrence and use 4 CPU cores for training. All these specifics are
translated in only one line of code.</p>
<p>Let’s train our model then:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>model <span class="op">=</span> Word2Vec([tokens_no_stopwords], vector_size<span class="op">=</span><span class="dv">300</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>, sg<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
<p>We can inspect already what’s the output of this training, by
checking the top 5 most similar words to “maan” (moon):</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>word_vectors.most_similar(<span class="st">'maan'</span>, topn<span class="op">=</span><span class="dv">5</span>)</span></code></pre>
</div>
<p><code>[('plek', 0.48467501997947693), ('ouders', 0.46935707330703735), ('supe|', 0.3929591178894043), ('rotterdam', 0.37788015604019165), ('verkeerden', 0.33672046661376953)]</code></p>
<p>We have trained our model on one page only of the newspaper and the
training was very quick. However, to approach our problem it’s best to
train our model on the entire dataset. We dont’ have the resources for
doing that on our local laptop, but luckily for us, <a href="https://zenodo.org/records/3237380" class="external-link">Wevers, M (2019)</a> did that
already for us and released it publicly. Let’s download this dataset on
our laptop and let’s save them in a folder called <code>w2v</code>.</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>folder_path <span class="op">=</span> <span class="st">'data/w2v/'</span></span></code></pre>
</div>
</section><section><h2 class="section-heading" id="load-the-embeddings-and-inspect-them">5. Load the embeddings and inspect them<a class="anchor" aria-label="anchor" href="#load-the-embeddings-and-inspect-them"></a>
</h2>
<hr class="half-width">
<p>We proceed to load our models. We will load all pre-trained model
files from the journal <code>telegraaf</code> into a list. The library
<code>gensim</code> contains a method called <code>KeyedVectors</code>
which allows us to load them.</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> KeyedVectors</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a>filenames_by_decade <span class="op">=</span> [</span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a>    <span class="st">'telegraaf_1950_1959.w2v'</span>,</span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a>    <span class="st">'telegraaf_1960_1969.w2v'</span>,</span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a>    <span class="st">'telegraaf_1970_1979.w2v'</span>,</span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a>    <span class="st">'telegraaf_1980_1989.w2v'</span></span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a>]</span>
<span id="cb31-10"><a href="#cb31-10" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" tabindex="-1"></a><span class="kw">def</span> load_w2v_models(filenames, folder_path):</span>
<span id="cb31-12"><a href="#cb31-12" tabindex="-1"></a>    loaded_models_by_decade <span class="op">=</span> []</span>
<span id="cb31-13"><a href="#cb31-13" tabindex="-1"></a>    </span>
<span id="cb31-14"><a href="#cb31-14" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> filenames:</span>
<span id="cb31-15"><a href="#cb31-15" tabindex="-1"></a>        path <span class="op">=</span> os.path.join(folder_path, <span class="bu">file</span>)</span>
<span id="cb31-16"><a href="#cb31-16" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'loading model </span><span class="sc">{</span>path<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb31-17"><a href="#cb31-17" tabindex="-1"></a>        </span>
<span id="cb31-18"><a href="#cb31-18" tabindex="-1"></a>        model <span class="op">=</span> KeyedVectors.load_word2vec_format(path, binary<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-19"><a href="#cb31-19" tabindex="-1"></a>        </span>
<span id="cb31-20"><a href="#cb31-20" tabindex="-1"></a>        loaded_models_by_decade.append(model)</span>
<span id="cb31-21"><a href="#cb31-21" tabindex="-1"></a>    <span class="cf">return</span> loaded_models_by_decade</span>
<span id="cb31-22"><a href="#cb31-22" tabindex="-1"></a></span>
<span id="cb31-23"><a href="#cb31-23" tabindex="-1"></a><span class="co"># run the function to load the models</span></span>
<span id="cb31-24"><a href="#cb31-24" tabindex="-1"></a>telegraaf_models <span class="op">=</span> load_w2v_models(filenames_by_decade, folder_path)</span></code></pre>
</div>
<p>We should see the following prints:</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a>loading model data<span class="op">/</span>w2v<span class="op">/</span>telegraaf_1950_1959.w2v</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>loading model data<span class="op">/</span>w2v<span class="op">/</span>telegraaf_1960_1969.w2v</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a>loading model data<span class="op">/</span>w2v<span class="op">/</span>telegraaf_1970_1979.w2v</span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>loading model data<span class="op">/</span>w2v<span class="op">/</span>telegraaf_1980_1989.w2v</span></code></pre>
</div>
<p>This means that we have loaded the models correctly.</p>
<p>Now let’s proceed to inspect the top 10 neighbours of the word
<code>mobiel</code> (to start with) across the decades:</p>
<div class="codewrapper sourceCode" id="cb33">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a></span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a>decades <span class="op">=</span> [<span class="st">'50s'</span>, <span class="st">'60s'</span>, <span class="st">'70s'</span>, <span class="st">'80s'</span>]</span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" tabindex="-1"></a><span class="cf">for</span> decade <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(decades)):</span>
<span id="cb33-5"><a href="#cb33-5" tabindex="-1"></a>    top_neighbours <span class="op">=</span> telegraaf_models[decade].most_similar(<span class="st">'mobiel'</span>, topn<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-6"><a href="#cb33-6" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'decade: </span><span class="sc">{</span>decades[decade]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb33-7"><a href="#cb33-7" tabindex="-1"></a>    <span class="cf">for</span> neighbour <span class="kw">in</span> top_neighbours:</span>
<span id="cb33-8"><a href="#cb33-8" tabindex="-1"></a>        <span class="bu">print</span>(neighbour[<span class="dv">0</span>])</span>
<span id="cb33-9"><a href="#cb33-9" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code></pre>
</div>
<p>This is what we should see:</p>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>decade: <span class="dv">50</span><span class="er">s</span></span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a>locatie</span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>stationeren</span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a>landingsvaartuigen</span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a>stationering</span>
<span id="cb34-6"><a href="#cb34-6" tabindex="-1"></a>toegangspoort</span>
<span id="cb34-7"><a href="#cb34-7" tabindex="-1"></a>legeren</span>
<span id="cb34-8"><a href="#cb34-8" tabindex="-1"></a>mustangs</span>
<span id="cb34-9"><a href="#cb34-9" tabindex="-1"></a>imperialistisch</span>
<span id="cb34-10"><a href="#cb34-10" tabindex="-1"></a>landmijnen</span>
<span id="cb34-11"><a href="#cb34-11" tabindex="-1"></a>oprukt</span>
<span id="cb34-12"><a href="#cb34-12" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" tabindex="-1"></a></span>
<span id="cb34-14"><a href="#cb34-14" tabindex="-1"></a>decade: <span class="dv">60</span><span class="er">s</span></span>
<span id="cb34-15"><a href="#cb34-15" tabindex="-1"></a>maagpijnen</span>
<span id="cb34-16"><a href="#cb34-16" tabindex="-1"></a>ureum</span>
<span id="cb34-17"><a href="#cb34-17" tabindex="-1"></a>bagagewagen</span>
<span id="cb34-18"><a href="#cb34-18" tabindex="-1"></a>waterpartij</span>
<span id="cb34-19"><a href="#cb34-19" tabindex="-1"></a>stillere</span>
<span id="cb34-20"><a href="#cb34-20" tabindex="-1"></a>boormachine</span>
<span id="cb34-21"><a href="#cb34-21" tabindex="-1"></a>achterportier</span>
<span id="cb34-22"><a href="#cb34-22" tabindex="-1"></a>doorsnijden</span>
<span id="cb34-23"><a href="#cb34-23" tabindex="-1"></a>stralingsgevaar</span>
<span id="cb34-24"><a href="#cb34-24" tabindex="-1"></a>opgepast</span>
<span id="cb34-25"><a href="#cb34-25" tabindex="-1"></a></span>
<span id="cb34-26"><a href="#cb34-26" tabindex="-1"></a></span>
<span id="cb34-27"><a href="#cb34-27" tabindex="-1"></a>decade: <span class="dv">70</span><span class="er">s</span></span>
<span id="cb34-28"><a href="#cb34-28" tabindex="-1"></a>beweeglijk</span>
<span id="cb34-29"><a href="#cb34-29" tabindex="-1"></a>beweeglijke</span>
<span id="cb34-30"><a href="#cb34-30" tabindex="-1"></a>kunstrubriek</span>
<span id="cb34-31"><a href="#cb34-31" tabindex="-1"></a>kleutert</span>
<span id="cb34-32"><a href="#cb34-32" tabindex="-1"></a>klankbeeld</span>
<span id="cb34-33"><a href="#cb34-33" tabindex="-1"></a>radiojournaal</span>
<span id="cb34-34"><a href="#cb34-34" tabindex="-1"></a>knipperlicht</span>
<span id="cb34-35"><a href="#cb34-35" tabindex="-1"></a>meisjeskoor</span>
<span id="cb34-36"><a href="#cb34-36" tabindex="-1"></a>kinderkoor</span>
<span id="cb34-37"><a href="#cb34-37" tabindex="-1"></a>volksverhalen</span>
<span id="cb34-38"><a href="#cb34-38" tabindex="-1"></a></span>
<span id="cb34-39"><a href="#cb34-39" tabindex="-1"></a></span>
<span id="cb34-40"><a href="#cb34-40" tabindex="-1"></a>decade: <span class="dv">80</span><span class="er">s</span></span>
<span id="cb34-41"><a href="#cb34-41" tabindex="-1"></a>communicatieapparatuur</span>
<span id="cb34-42"><a href="#cb34-42" tabindex="-1"></a>parkeerterreinen</span>
<span id="cb34-43"><a href="#cb34-43" tabindex="-1"></a>sonar</span>
<span id="cb34-44"><a href="#cb34-44" tabindex="-1"></a>alarmsysteem</span>
<span id="cb34-45"><a href="#cb34-45" tabindex="-1"></a>gasinstallaties</span>
<span id="cb34-46"><a href="#cb34-46" tabindex="-1"></a>lichtnet</span>
<span id="cb34-47"><a href="#cb34-47" tabindex="-1"></a>elektromotor</span>
<span id="cb34-48"><a href="#cb34-48" tabindex="-1"></a>inentingen</span>
<span id="cb34-49"><a href="#cb34-49" tabindex="-1"></a>sensoren</span>
<span id="cb34-50"><a href="#cb34-50" tabindex="-1"></a>hulpbehoevenden</span></code></pre>
</div>
<p>Let’s inspect together these results:</p>
<p>In the 50s, the neighbouring words predominantly point towards
military and geopolitical terms (stationeren, landingsvaartuigen,
stationering, legeren, and landmijnen). The presence of the word
<em>imperialistisch</em> also suggests discussions about imperialism,
possibly reflecting post WWII tensions (in the 50s Europe was entering
Cold War period).</p>
<p>In the 60s, the term is associated to meanings related to health,
safety and mechanical terms. <em>Stralingsgevaar</em> and
<em>opgepast</em> suggest concerns about radiation and the need for
caution, possibly reflecting the nuclear anxieties of the era.</p>
<p>In the 70s the word is associated to technological advancement and
culture. While finally in the 80s we see a list of words that have solid
grounds in technological and infrastracture terms. Words like
<em>communicatieapparatuur</em>, <em>sonar</em>, <em>alarmsysteem</em>,
<em>elektromotor</em>, and <em>sensoren</em> signals the push that
technology has had in this period, with the advent of mobile phones
(communicatieapparatuur).</p>
<p>All in all, the word’s meaning evolved from being a means of
transport to a modern technology tool employed in urban infrastructure,
societal well-being and communication.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Reproduce the steps above for the other words: <code>televisie</code>
and <code>ijzeren</code>. What do you expect from their historical
semantic evolution? Television was already present in the 50s, although
the technology around it has evolved up to the 1989. And what about the
term <code>iron</code>? When do you expect this term to acquire a
meaning related to the Cold War e.g. <a href="https://en.wikipedia.org/wiki/Iron_Curtain" class="external-link">Iron Curtain</a>?</p>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-02-transformers"><p>Content from <a href="02-transformers.html">Episode 2: BERT and Transformers</a></p>
<hr>
<p>Last updated on 2025-01-09 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are Transformers?</li>
<li>What is BERT and how does it work?</li>
<li>How can I use BERT as a text classifier?</li>
<li>How should I evaluate my classifiers?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul>
<li>Understand how a Transformer works and recognize their different use
cases.</li>
<li>Use pre-trained transformers language models (e.g. BERT) to classify
texts.</li>
<li>Use a pre-trained transformer Named Entity Recognizer.</li>
<li>Understand assumptions and basic evaluation for NLP outputs.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>In the previous lesson we learned how Word2Vec can be used to
represent words as vectors. Having these representations allows us to
apply operations directly on the vectors that have numerical properties
that can be mapped to some syntactic and semantic properties of words;
such as the cases of analogies or finding synonyms. Once we transform
words into vectors, these can also be used as <strong>features</strong>
for classifiers that can be trained predict any supervised NLP task.</p>
<p>The main drawback of Word2Vec is that each word is represented in
isolation, and unfortunately that is not how language works. Words get
their meanings based on the specific context in which they are used
(take for example polysemy, the cases where the same word can have very
different meanings depending on the context); therefore, we would like
to have richer vector representations of words that also integrate
context into account in order to obtain more powerful
representations.</p>
<p>In 2019, the BERT language model was introduced using a novel
architecture called Transformer (2017), which allowed precisely to
integrate words’ context into representations. To understand BERT, we
will first look at what a transformer is and we will then directly use
some code to make use of BERT.</p>
<div class="section level1">
<h1 id="transformers">Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a>
</h1>
<p>Every text can be seen as a sequence of sentences and likewise each
sentence can be seen as a sequence of tokens (we use the term
<em>token</em> instead of <em>word</em> because it is more general:
tokens can be words, punctuation symbols, numbers, or even sub-words).
Traditionally Recurrent Neural Networks (RNNs; and later their fancy
version, LSTMs) were used to tackle token and sentence classification
problems to account for the interdependencies inherent to sequences of
symbols (i.e. sentences). RNNs were in theory powerful enough to capture
these dependencies, something that is very valuable when dealing with
language, but in practice they were resource consuming (both in training
time and computational resources) and also the longer the sequences got,
the harder it was to capture long-distance dependencies succesfully.</p>
<p>The Transformer is a neural network architecture proposed by Google
researchers <a href="https://arxiv.org/pdf/1706.03762" class="external-link">in 2017</a> to
address these and other limitations of RNNs and LSTMs. In their paper,
<em>Attention is all you Need</em>, they tackled specifically the
problem of Machine Translation (MT), which in NLP terms is stated as:
how to generate a sentence (sequence of words) in target language B
given a sentence in source language A? In order to translate, first one
neural network needs to <em>encode</em> the meaning of the source
language A into vector representations, and then a second neural network
needs to <em>decode</em> that representation into tokens that are
understandable in language B. Therefore translation is modeling language
B <em>conditioned</em> on what language A originally said.</p>
<figure><img src="fig/trans1.png" alt="Transformer Architecture" class="figure mx-auto d-block"><div class="figcaption">Transformer Architecture</div>
</figure><p>As seen in the picture, the original Transformer is an
Encoder-Decoder network that tackles translation. We first need a token
embedder which converts the string of words into a sequence of vectors
that the Transformer network can process. The first component, the
<strong>Encoder</strong>, is optimized for creating rich representations
of the source sequence (in this case an English sentence) while the
second one, the <strong>Decoder</strong> is a generative network that is
conditioned on the encoded representation and, with the help of the
attention mechanism, generates the most likely token in the target
sequence (in this case Dutch words) based on both the tokens generated
so far and the full initial English context.</p>
<p>Next, we will see how BERT exploits the idea of a Transformer Encoder
to generate powerful word representations.</p>
</div>
<div class="section level1">
<h1 id="bert">BERT<a class="anchor" aria-label="anchor" href="#bert"></a>
</h1>
<p><a href="https://aclanthology.org/N19-1423.pdf" class="external-link">BERT</a> is an
acronym that stands for <strong>B</strong>idirectional
<strong>E</strong>ncoder <strong>R</strong>epresentations from
<strong>T</strong>ransformers. The name describes it all: the idea is to
use the power of the Encoder component of the Transformer architecture
to create powerful token representations that preserve the contextual
meaning of the whole input segment. The BERT vector representations of
each token take into account both the left context (what comes before
the word) and the right context (what comes after the word). Another
advantage of the transformer Encoder is that it is parallelizable, which
made it posible for the first time to train these networks on millions
of datapoints, dramatically improving model generalization.</p>
<div id="pretraining-bert" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="pretraining-bert" class="callout-inner">
<h3 class="callout-title">Pretraining BERT</h3>
<div class="callout-content">
<p>To obtain the BERT vector representations the Encoder is pre-trained
with two different tasks: - <strong>Masked Language Model:</strong> for
each sentence, mask one token at a time and predict which token is
missing based on the context from both sides. A training input example
would be “Maria [MASK] Groningen” and the model should predict the word
“loves”. - <strong>Next Sentence Prediction:</strong> the Encoder gets a
linear binary classifier on top, which is trained to decide for each
pair of sequences A and B, if sequence A precedes sequence B in a text.
For the sentence pair: “Maria loves Groningen.” and “This is a city in
the Netherlands.” the output of the classifier is “True” and for the
pair “Maria loves Groningen.” and “It was a tasty cake.” the output
should be “false” as there is no obvious continuation between the two
sentences.</p>
<p>Already the second pre-training task gives us an idea of the power of
BERT: after it has been pretrained on hundreds of thousands of texts,
one can plug-in a classifier on top and re-use the <em>linguistic</em>
knowledge previously acquired to fine-tune it for a specific task,
without needing to learn the weights of the whole network from scratch
all over again. In the next sections we will describe the components of
BERT and show how to use it. This model and hundreds of related
transformer-based pre-trained encoders can also be found on <a href="https://huggingface.co/google-bert/bert-base-cased" class="external-link">Hugging
Face</a>.</p>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="bert-architecture">BERT Architecture<a class="anchor" aria-label="anchor" href="#bert-architecture"></a>
</h1>
<p>Now that we used the BERT language model component we can dive into
the architecture of BERT to understand it better.</p>
<p>As in any basic NLP pipeline, the first step is to pre-process the
raw text so it is ready to be fed into the Transformer. Tokenization in
BERT does not happen at the word-level but rather splits texts into what
they call WordPieces (the reason for this decision is complex, but in
short, researchers found that splitting <em>human words</em> into
<em>subtokens</em> exploits better the character sub-sequences inside
words and helps the model converge faster). A word then sometimes is
decomposed into one or several (sub) tokens.</p>
<ol style="list-style-type: decimal">
<li>
<strong>Tokenizer:</strong> splits text into tokens that the model
recognizes</li>
<li>
<strong>Embedder:</strong> converts each token into a fixed-sized
vector that represents it. These vectors are the actual input for the
Encoder.</li>
<li>
<strong>Encoder</strong> several neural layers that model the
token-level interactions of the input sequence to enhance meaning
representation. The output of the encoder is a set of
<strong>H</strong>idden layers, the vector representation of the
ingested sequence.</li>
<li>
<strong>Output Layer:</strong> the final encoder layer (which we
depict as a sequence <strong>H</strong>’s in the figure) contains
arguably the best token-level representations that encode syntactic and
semantic properties of each token, but this time each vector is already
contextualized with the specific sequence.</li>
<li>
<em>OPTIONAL</em> <strong>Classifier Layer:</strong> an additional
classifier can be connected on top of the BERT token vectors which are
used as features for performing a downstream task. This can be used to
classify at the text level, for example sentiment analysis of a
sentence, or at the token-level, for example Named Entity
Recognition.</li>
</ol>
<figure><img src="fig/bert3.png" alt="BERT Architecture" class="figure mx-auto d-block"><div class="figcaption">BERT Architecture</div>
</figure><div class="section level2">
<h2 id="bert-code">BERT Code<a class="anchor" aria-label="anchor" href="#bert-code"></a>
</h2>
<p>Let’s see how these components can be manipulated with code. For this
we will be using the HugingFace’s <em>transformers</em> python library.
We can install it with:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="ex">pip</span> install transformers</span></code></pre>
</div>
<p>The first two main components we need to initialize are the model and
tokenizer. The HuggingFace hub contains thousands of models based on a
Transformer architecture for dozens of tasks, data domains and also
hundreds of languages. Here we will explore the vanilla English BERT
which was how everything started. We can initialize this model with the
next lines:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-cased'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">"bert-base-cased"</span>)</span></code></pre>
</div>
</div>
<div class="section level2">
<h2 id="bert-tokenizer">BERT Tokenizer<a class="anchor" aria-label="anchor" href="#bert-tokenizer"></a>
</h2>
<p>We start with a string of text as written in any blog, book,
newspaper etcetera. The <code>tokenizer</code> object is responsible of
splitting the string into recognizable tokens for the model and
embedding the tokens into their vector representations</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Maria loves Groningen"</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>encoded_input <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="bu">print</span>(encoded_input)</span></code></pre>
</div>
<p>The print shows the <code>encoded_input</code> object returned by the
tokenizer, with its attributes and values. The <code>input_ids</code>
are the most important output for now, as these are the token IDs
recognized by BERT</p>
<pre><code>{
    'input_ids': tensor([[  101,  3406,  7871,   144,  3484, 15016,   102]]),
    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]),
    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])
}
</code></pre>
<p>NOTE: the printing function shows transformers objects as
dictionaries; however, to access the attributes, you must use the python
object syntax, such as in the following example:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="bu">print</span>(encoded_input.input_ids.shape)</span></code></pre>
</div>
<p>Output:</p>
<p><code>torch.Size([1, 7])</code></p>
<p>The output is a 2-dimensional tensor where the first dimention
contains 1 element (this dimension represents the batch size), and the
second dimension contains 7 elements which are equivalent to the 7
tokens that BERT generated with our string input.</p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>You noticed in the previous outputs the <code>tensor()</code> and
<code>torch()</code> wrappers around the arrays of integers. This is
showing that the <code>transformers</code> library uses
<code>pytorch</code> underneath, one of the most popular libraries for
Deep Learning in Python. Pytorch’s basic unit is the Tensor.</p>
<p>A <em>tensor</em> is a generalization of a multidimentional array of
data. By convention, a vector is a 1-dimensional sequence of scalar
numbers (or a 1-dim tensor), a matrix is a 2-dimensional sequence (2-dim
tensor) and for N-dimensions where N &gt; 2 we use the concept of
tensor.</p>
</div>
</div>
</div>
<p>In order to see what these Token IDs represent, we can
<em>translate</em> them into human readable strings. This includes
converting the tensors into numpy arrays and converting each ID into its
string representation:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_input.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>string_tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"IDs:"</span>, token_ids)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TOKENS:"</span>, string_tokens)</span></code></pre>
</div>
<p><code>IDs: [101, 3406, 7871, 144, 3484, 15016, 102]</code></p>
<p><code>TOKENS: ['[CLS]', 'Maria', 'loves', 'G', '##ron', '##ingen', '[SEP]']</code></p>
<p>These show us the WordPieces that the BERT Encoder will receive and
process. We will look more in detail into the tokenization and special
tokens later. For now, you just need to know that the encoder uses this
token IDs to retrieve the corresponding embedding vector from its
vocabulary, the string representations are just for the human
reader.</p>
</div>
<div class="section level2">
<h2 id="bert-output-object">BERT Output Object<a class="anchor" aria-label="anchor" href="#bert-output-object"></a>
</h2>
<p>To give a forward pass of the Encoder and obtain the vector
representations, we pass the <code>encoded_input</code> object generated
by the tokenizer.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(output)</span></code></pre>
</div>
<p>The <code>output</code> variable in this case stores an ModelOutput
object, which contains a handful of values:</p>
<pre><code>BaseModelOutputWithPoolingAndCrossAttentions(
    last_hidden_state=tensor([[
        [6.3959e-02, -4.8466e-03, -8.4682e-02,  ..., -2.8042e-02, 4.3824e-01,  2.0693e-02],
        [-3.7276e-04, -2.0076e-01,  2.5096e-01,  ...,  9.9699e-01, -5.4226e-01,  1.7926e-01],
        ...
        [ 7.1929e-01, -1.1457e-01,  1.4804e-01,  ...,  5.3051e-01, 7.4839e-01,  7.8224e-02]
    ]]),
    pooler_output=tensor([[-0.6889,  0.4869,  0.9998, -0.9888,  0.9296,  0.8637, ...,  1.0000, -0.7488,  0.9860]]),
    hidden_states=None,
    past_key_values=None,
    attentions=None,
    cross_attentions=None
)</code></pre>
<p>We must focus for now on the <code>last_hidden_state</code> field,
which contains the last layer vector of weights for each token, arguably
the best contextualized representation of the token.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="bu">print</span>(output.last_hidden_state.shape)</span></code></pre>
</div>
<p><code>torch.Size([1, 7, 768])</code></p>
<p>When we print the shape of this field, we obtain again a Pytorch
Tensor: <code>torch.Size([1, 7, 768])</code>. This time, the first
dimension is the batch size, the second is the number of tokens (we have
7 tokens for this example as seen before), and the third, the
dimensionality of the vectors. In the case of BERT-base each token
vector always has a shape of 768. As opposed to the previous tensor,
each of the 7 tokens are not just one integer anymore, but a whole
vector of weights, hence the 3-dimensionality of the tensor.</p>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>When running examples in a BERT pre-trained model, it is advisable to
wrap your code inside a <code>torch.no_grad():</code> context. This is
linked to the fact that BERT is a Neural Network that has been trained
(and can be further finetuned) with the Backpropagation algorithm.
Essentially, this wrapper tells the model that we are not in training
mode, and we are not interested in <em>updating</em> the weights (as it
would happen when training any neural network), because the weights are
already optimal enough. By using this wrapper, we make the model more
efficient as it does not need to calculate the gradients for an eventual
backpropagation step, since we are only interested in what <em>comes
out</em> of the Encoder. So the previous code can be made more efficient
like this:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    <span class="bu">print</span>(output)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    <span class="bu">print</span>(output.last_hidden_state.shape)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="bert-as-a-language-model">BERT as a Language Model<a class="anchor" aria-label="anchor" href="#bert-as-a-language-model"></a>
</h1>
<p>Now that we know how to embedd and run the model to obtain the
representations, we can test the code for our first NLP Task: Language
Modelling (LM). As mentioned before, the main pre-training task of BERT
is LM: calculating the probability of a word based on the known
neighboring words (yes, Word2Vec was also a kind of LM). Obtaining
training data for this task is very cheap, as all we need is millions of
sentences from existing texts, without any labels. In this setting, BERT
encodes a sequence of words, and predicts from a set of English tokens,
what is the most likely token that could be inserted in the
<code>[MASK]</code> position</p>
<figure><img src="fig/bert1b.png" alt="BERT Language Modeling" class="figure mx-auto d-block"><div class="figcaption">BERT Language Modeling</div>
</figure><p>We can therefore start using BERT as a predictor for word completion,
and the word can be in any position inside the sentence. We will also
learn here how to use the <code>pipeline</code> object, this is very
useful when we only want to use a pre-trained model for predictions (no
need to fine-tune). The <code>pipeline</code> will internally initialize
both model and tokenizer for us. In this case again we use
<code>bert-base-cased</code>, which refers to the vanilla BERT English
model. Once we declared a pipeline, we can feed it with sentences that
contain one masked token at a time (beware that BERT can only predict
one word at a time, since that was its training scheme). For
example:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="kw">def</span> pretty_print_outputs(sentences, model_outputs):</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="cf">for</span> i, model_out <span class="kw">in</span> <span class="bu">enumerate</span>(model_outputs):</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=====</span><span class="ch">\t</span><span class="st">"</span>,sentences[i])</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>        <span class="cf">for</span> label_scores <span class="kw">in</span> model_out:</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>            <span class="bu">print</span>(label_scores)</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>nlp <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"fill-mask"</span>, model<span class="op">=</span><span class="st">"bert-base-cased"</span>, tokenizer<span class="op">=</span><span class="st">"bert-base-cased"</span>)</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"Paris is the [MASK] of France"</span>, <span class="st">"I want to eat a cold [MASK] this afternoon"</span>, <span class="st">"Maria [MASK] Groningen"</span>]</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>model_outputs <span class="op">=</span> nlp(sentences, top_k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 Paris is the [MASK] of France
{'score': 0.9807965755462646, 'token': 2364, 'token_str': 'capital', 'sequence': 'Paris is the capital of France'}
{'score': 0.004513159394264221, 'token': 6299, 'token_str': 'Capital', 'sequence': 'Paris is the Capital of France'}
{'score': 0.004281804896891117, 'token': 2057, 'token_str': 'center', 'sequence': 'Paris is the center of France'}
{'score': 0.002848200500011444, 'token': 2642, 'token_str': 'centre', 'sequence': 'Paris is the centre of France'}
{'score': 0.0022805952467024326, 'token': 1331, 'token_str': 'city', 'sequence': 'Paris is the city of France'}

=====	 I want to eat a cold [MASK] this afternoon
{'score': 0.19168031215667725, 'token': 13473, 'token_str': 'pizza', 'sequence': 'I want to eat a cold pizza this afternoon'}
{'score': 0.14800849556922913, 'token': 25138, 'token_str': 'turkey', 'sequence': 'I want to eat a cold turkey this afternoon'}
{'score': 0.14620967209339142, 'token': 14327, 'token_str': 'sandwich', 'sequence': 'I want to eat a cold sandwich this afternoon'}
{'score': 0.09997560828924179, 'token': 5953, 'token_str': 'lunch', 'sequence': 'I want to eat a cold lunch this afternoon'}
{'score': 0.06001955270767212, 'token': 4014, 'token_str': 'dinner', 'sequence': 'I want to eat a cold dinner this afternoon'}

=====	 Maria [MASK] Groningen
{'score': 0.24399833381175995, 'token': 117, 'token_str': ',', 'sequence': 'Maria, Groningen'}
{'score': 0.12300779670476913, 'token': 1104, 'token_str': 'of', 'sequence': 'Maria of Groningen'}
{'score': 0.11991506069898605, 'token': 1107, 'token_str': 'in', 'sequence': 'Maria in Groningen'}
{'score': 0.07722211629152298, 'token': 1306, 'token_str': '##m', 'sequence': 'Mariam Groningen'}
{'score': 0.0632941722869873, 'token': 118, 'token_str': '-', 'sequence': 'Maria - Groningen'}
</code></pre>
<p>When we call the <code>nlp</code> pipeline, requesting to return the
<code>top_k</code> most likely suggestions to complete the provided
sentences (in this case <code>k=5</code>). The pipeline returns a list
of outputs as python dictionaries. Depending on the task, the fields of
the dictionary will differ. In this case, the <code>fill-mask</code>
task returns a score (between 0 and 1, the higher the score the more
likely the token is), a tokenId, and its corresponding string, as well
as the full “unmasked” sequence.</p>
<p>In the list of outputs we can observe: the first example shows
correctly that the missing token in the first sentence is
<em>capital</em>, the second example is a bit more ambiguous, but the
model at least uses the context to correctly predict a series of items
that can be eaten (unfortunately, none of its suggestions sound very
tasty); finally, the third example gives almost no useful context so the
model plays it safe and only suggests prepositions or punctuation. This
already shows some of the weaknesses of the approach.</p>
<p>We will next see the case of combining BERT with a classifier on
top.</p>
</div>
<div class="section level1">
<h1 id="bert-for-text-classification">BERT for Text Classification<a class="anchor" aria-label="anchor" href="#bert-for-text-classification"></a>
</h1>
<p>The task of text classification is assigning a label to a whole
sequence of tokens, for example a sentence. With the parameter
<code>task="text_classification"</code> the <code>pipeline()</code>
function will load the base model and automatically add a linear layer
with a softmax on top. This layer can be fine-tuned with our own labeled
data or we can also directly load the fully pre-trained text
classification models that are already available in HuggingFace.</p>
<figure><img src="fig/bert4.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure><p>Let’s see the example of a ready pre-trained emotion classifier based
on <code>RoBERTa</code> model. This model was fine-tuned in the Go
emotions <a href="https://huggingface.co/datasets/google-research-datasets/go_emotions" class="external-link">dataset</a>,
taken from English Reddit and labeled for 28 different emotions at the
sentence level. The fine-tuned model is called <a href="https://huggingface.co/SamLowe/roberta-base-go_emotions" class="external-link">roberta-base-go_emotions</a>.
This model takes a sentence as input and ouputs a probability
distribution over the 28 possible emotions that might be conveyed in the
text. For example:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"text-classification"</span>, model<span class="op">=</span><span class="st">"SamLowe/roberta-base-go_emotions"</span>, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"I am not having a great day"</span>, <span class="st">"This is a lovely and innocent sentence"</span>, <span class="st">"Maria loves Groningen"</span>]</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>model_outputs <span class="op">=</span> classifier(sentences)</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 I am not having a great day
{'label': 'disappointment', 'score': 0.46669483184814453}
{'label': 'sadness', 'score': 0.39849498867988586}
{'label': 'annoyance', 'score': 0.06806594133377075}

=====	 This is a lovely and innocent sentence
{'label': 'admiration', 'score': 0.6457845568656921}
{'label': 'approval', 'score': 0.5112180113792419}
{'label': 'love', 'score': 0.09214121848344803}

=====	 Maria loves Groningen
{'label': 'love', 'score': 0.8922032117843628}
{'label': 'neutral', 'score': 0.10132959485054016}
{'label': 'approval', 'score': 0.02525361441075802}</code></pre>
<p>This code outputs again a list of dictionaries with the
<code>top-k</code> (<code>k=3</code>) emotions that each of the two
sentences convey. In this case, the first sentence evokes (in order of
likelihood) <em>dissapointment</em>, <em>sadness</em> and
<em>annoyance</em>; whereas the second sentence evokes <em>love</em>,
<em>neutral</em> and <em>approval</em>. Note however that the likelihood
of each prediction decreases dramatically below the top choice, so
perhaps this specific classifier is only useful for the top emotion.</p>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>Finetunning BERT is very cheap, because we only need to train the
<em>classifier</em> layer, a very small neural network, that can learn
to choose between the classes (labels) for your custom classification
problem, without needing a big amount of annotated data. This classifier
is just a one-layer neural layer with a softmax that assigns a score
that can be translated to the probability over a set of labels, given
the input features provided by BERT, which <em>encodes</em> the meaning
of the entire sequence in its hidden states.</p>
</div>
</div>
</div>
<figure><img src="fig/bert4b.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure>
</div>
<div class="section level1">
<h1 id="understanding-bert-architecture">Understanding BERT Architecture<a class="anchor" aria-label="anchor" href="#understanding-bert-architecture"></a>
</h1>
<p>This will help to understand some of the strengths and weaknesses of
using BERT-based classifiers.</p>
<div class="section level2">
<h2 id="tokenizer-and-embedder">Tokenizer and Embedder<a class="anchor" aria-label="anchor" href="#tokenizer-and-embedder"></a>
</h2>
<p>Let’s revisit the tokenizer to better grasp how it is working. The
tokenization step might seem trivial but in reality models’ tokenizers
make a big difference in the final results of your classifiers,
depending on the task you are trying to solve. Understanding the
tokenizer of each model (as well as the model type!) can save us a lot
of debugging when we work with our custom problem.</p>
<p>We will feed again a sentence into the tokenizer to observe how it
outputs a sequence of vectors (also called a <em>tensor</em>: by
convention, a vector is a sequence of scalar numbers, a matrix is a
2-dimensional sequence and a tensor is a N-dimensional sequence of
numbers), each one of them representing a wordPiece:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co"># Feed text into the tokenizer </span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Maria's passion for music is clearly heard in every note and every enchanting melody."</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>encoded_input <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_input.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>string_tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="bu">print</span>(string_tokens)</span></code></pre>
</div>
<p><code>['[CLS]', 'Maria', "'", 's', 'passion', 'for', 'music', 'is', 'clearly', 'heard', 'in', 'every', 'note', 'and', 'every', 'en', '##chan', '##ting', 'melody', '.', '[SEP]']</code></p>
<p>This shows a list of token IDs, as we saw with our first example,
this time the list consists of 21 BERT tokens.</p>
<p>When inspecting the string tokens, we see that most “words” were
converted into a single token, however <em>enchanting</em> was splitted
into three sub-tokens: <code>'en', '##chan', '##ting'</code> the
hashtags indicate wether a sub-token was part of a bigger word or not,
this is useful to recover the human-readable strings later. The
<code>[CLS]</code> token was added at a beginning and is intended to
represent the meaning of the whole sequence, likewise the
<code>[SEP]</code> token was added to indicate that it is where the
sentence ends.</p>
<p>The next step is to give the sequence of tokens to the Encoder which
processes it through the transformer layers and outputs a sequence of
dense vectors:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>    <span class="bu">print</span>(output.last_hidden_state.shape)</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    <span class="bu">print</span>(output.last_hidden_state[<span class="dv">0</span>][<span class="dv">0</span>])</span></code></pre>
</div>
<p><code>torch.Size([1, 21, 768])</code></p>
<pre><code>tensor([-5.3755e-02, -1.1100e-01, -8.8204e-02, -1.1233e-01,  8.1979e-02,
        -7.2656e-03,  2.5323e-01, -3.0361e-01,  1.7344e-01, -1.1212e+00, ...    </code></pre>
<p>We chose to print here the vector representation of
<code>[CLS]</code>: by indexing the <code>last_hidden_state[0]</code> we
access to the first batch (21 vectors of 768-dimensionality), and by
again indexing <code>last_hidden_state[0][0]</code> we access the first
of the last_hidden_vectors, which as we saw in the token strings, belong
to <code>[CLS]</code> and is there to represent the whole sequence. We
only see a lot of fine-tuned weights which are not very informative in
their own, but the full-vectors are meaningful within the embedding
space, which emulates some aspects of linguistic meaning.</p>
<div id="callout5" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>In the case of wanting to obtain a single vector for
<em>enchanting</em>, you can average the three vectors that belong to
the token pieces that ultimately form that word. For example:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>tok_en <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">15</span>].detach().numpy()</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>tok_chan <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">16</span>].detach().numpy()</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>tok_ting <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">17</span>].detach().numpy()</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>tok_enchanting <span class="op">=</span> np.mean([tok_en, tok_chan, tok_ting], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>tok_enchanting.shape</span></code></pre>
</div>
<p>We use the functions <code>detach().numpy()</code> to bring the
values from the Pytorch execution environment (for example a GPU) into
the main python thread and treat it as a numpy vector for convenvience.
Then, since we are dealing with three numpy vectors we can average the
three of them and end op with a single <code>enchanting</code> vector of
768-dimensions representing the average of
<code>'en', '##chan', '##ting'</code>.</p>
</div>
</div>
</div>
<p>We can use the same method to encode two other sentences containing
the word <em>note</em> to see how BERT actually handles polysemy
(<em>note</em> means something very different in each sentence) thanks
to the representation of each word now being contextualized instead of
isolated as was the case with word2vec.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="co"># Search for the index of 'note' and obtain its vector from the sequence</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>note_index_1 <span class="op">=</span> string_tokens.index(<span class="st">"note"</span>)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>note_vector_1 <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][note_index_1].detach().numpy()</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>note_token_id_1 <span class="op">=</span> token_ids[note_index_1]</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a><span class="bu">print</span>(note_index_1, note_token_id_1, string_tokens)</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a><span class="bu">print</span>(note_vector_1[:<span class="dv">5</span>])</span></code></pre>
</div>
<p>We are basically printing the tokenized sentence from the previous
example and showing the index of the token <code>note</code> in the list
of tokens. We are also printing the tokenID assigned to this token and
the list of tokens. Finally, the last print shows the first five
dimensions of the vector representing the token <code>note</code>.</p>
<pre><code>12 3805 ['[CLS]', 'Maria', "'", 's', 'passion', 'for', 'music', 'is', 'clearly', 'heard', 'in', 'every', 'note', 'and', 'every', 'en', '##chan', '##ting', 'melody', '.', '[SEP]']
[0.15780845 0.38866335 0.41498923 0.03389652 0.40278202]</code></pre>
<p>Let’s encode now another sentence, also containing the word
<code>note</code>, and confirm that the same token string, with the same
assigned tokenID holds a vector with different weights:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="co"># Encode and then take the 'note' token from the second sentence</span></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>note_text_2 <span class="op">=</span> <span class="st">"I could not buy milk in the supermarket because the bank note I wanted to use was fake."</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>encoded_note_2 <span class="op">=</span> tokenizer(note_text_2, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_note_2.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>string_tokens_2 <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>note_index_2 <span class="op">=</span> string_tokens_2.index(<span class="st">"note"</span>)</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>note_vector_2 <span class="op">=</span> model(<span class="op">**</span>encoded_note_2).last_hidden_state[<span class="dv">0</span>][note_index_2].detach().numpy()</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>note_token_id_2 <span class="op">=</span> token_ids[note_index_2]</span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a><span class="bu">print</span>(note_index_2, note_token_id_2, string_tokens_2)</span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a><span class="bu">print</span>(note_vector_2[:<span class="dv">5</span>])</span></code></pre>
</div>
<pre><code>12 3805 ['[CLS]', 'I', 'could', 'not', 'buy', 'milk', 'in', 'the', 'supermarket', 'because', 'the', 'bank', 'note', 'I', 'wanted', 'to', 'use', 'was', 'fake', '.', '[SEP]']
[ 0.5003222   0.653664    0.22919582 -0.32637975  0.52929205]</code></pre>
<p>To be sure, we can compute the cosine similarity of the word
<em>note</em> in the first sentence and the word <em>note</em> in the
second sentence confirming that they are indeed two different
representations, even when in both cases they have the same token-id and
they are the 12th token of the sentence:</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>vector1 <span class="op">=</span> np.array(note_vector_1).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>vector2 <span class="op">=</span> np.array(note_vector_2).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity(vector1, vector2)</span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine Similarity 'note' vs 'note': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>With this small experiment, we have confirmed that the Encoder
produces context-dependent word representations, ass opposed to
Word2Vec, where <em>note</em> would always have the same vector no
matter where it appeared.</p>
</div>
<div class="section level2">
<h2 id="the-attention-mechanism">The Attention Mechanism<a class="anchor" aria-label="anchor" href="#the-attention-mechanism"></a>
</h2>
<p>The original attention mechanism (remember this was developed for
language translation) is a component in between the Encoder and the
Decoder that helps the model to <em>align</em> the important information
from the input sequence in order to generate a more accurate token in
the output sequence:</p>
<figure><img src="fig/trans3.png" alt="The Encoder-Decoder Attention Mechanism" class="figure mx-auto d-block"><div class="figcaption">The Encoder-Decoder Attention Mechanism</div>
</figure><p>In the example above, the attention puts more weight in the input
<em>Groningen</em>, so the decoder uses that information to
<em>know</em> that is should generate <em>Groningen</em>. Note that if
the decoder based it’s next word probability just on the sequence “Maria
houdt van …”, it could basically generate any word and still sound
natural. However, it is thanks to the attention mechanism that it
preserves the meaning of the input sequence.</p>
<p>Attention is a neural layer, therefore it can also be plugged-in
within the Encoder, this is called <strong>self-attention</strong> since
the mechanism will look at the interactions between the input sequence
itself (measure inportance between input sequence tokens vs input
sequence tokens). This is how BERT uses (self-) attention, which is very
useful to capture longer-range word dependencies such as correference,
where, for example, a pronoun can be linked to the noun it refers to
previously in the same sentence. See the following example:</p>
<figure><img src="fig/trans5.png" alt="The Encoder Self-Attention Mechanism" class="figure mx-auto d-block"><div class="figcaption">The Encoder Self-Attention Mechanism</div>
</figure><p>There are two sentences, in each one the pronoun “it” refers to a
different noun, “animal” or “street”, and this is completely depending
on the sentence context. Thanks to the self-attention BERT relates the
pronoun to its relevant correferent.</p>
<p>For this reason BERT is not only useful as a text classifier but also
for individual token classification tasks.</p>
</div>
</div>
<div class="section level1">
<h1 id="bert-for-token-classification">BERT for Token Classification<a class="anchor" aria-label="anchor" href="#bert-for-token-classification"></a>
</h1>
<p>Just as we plugged in a trainable text classifier layer, we can add a
token-level classifier that assigns a class to each of the tokens
encoded by a transformer (as opposed to one label for the whole
sequence). A specific example of this task is Named Entity Recognition,
but you can basically define any task that requires to
<em>highlight</em> sub-strings of text and classify them using this
technique.</p>
<div class="section level2">
<h2 id="named-entity-recognition">Named Entity Recognition<a class="anchor" aria-label="anchor" href="#named-entity-recognition"></a>
</h2>
<p>Named Entity Recognition (NER) is the task of recognizing mentions of
real-world entities inside a text. The concept of
<strong>Entity</strong> includes proper names that unequivocally
identify a unique individual (PER), place (LOC), organization (ORG), or
other object/name (MISC). Depending on the domain, the concept can
expanded to recognize other unique (and more conceptual) entities such
as DATE, MONEY, WORK_OF_ART, DISEASE, PROTEIN_TYPE, etcetera…</p>
<p>In terms of NLP, this boils down to classifying each token into a
series of labels (<code>PER</code>, <code>LOC</code>, <code>ORG</code>,
<code>O</code>[no-entity] ). Since a single entity can be expressed with
multiple words (e.g. New York) the usual notation used for labeling the
text is IOB (<strong>I</strong>nner <strong>O</strong>ut
<strong>B</strong>eginnig of entity) notations which identifies the
limits of each entity tokens. For example:</p>
<figure><img src="fig/bert5.png" alt="BERT as an NER Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an NER Classifier</div>
</figure><p>This is a typical sequence classification problem where an imput
sequence must be fully mapped into an output sequence of labels with
global constraints (for example, there can’t be an inner I-LOC label
before a beginning B-LOC label). Since the labels of the tokens are
context dependent, a language model with attention mechanism such as
BERT is very beneficial for a task like NER.</p>
<p>Because this is one of the core tasks in NLP, there are dozens of
pre-trained NER classifiers in HuggingFace that you can use right away.
We use once again the <code>pipeline()</code> to run the model for
predictions in your custom data, in this case with
<code>task="ner"</code>. For example:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForTokenClassification</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb24-11"><a href="#cb24-11" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb24-12"><a href="#cb24-12" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code prints the following:</p>
<pre><code>{'entity': 'B-PER', 'score': 0.9996068, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}
{'entity': 'I-PER', 'score': 0.999582, 'index': 5, 'word': 'Sc', 'start': 20, 'end': 22}
{'entity': 'I-PER', 'score': 0.9990482, 'index': 6, 'word': '##hm', 'start': 22, 'end': 24}
{'entity': 'I-PER', 'score': 0.9951691, 'index': 7, 'word': '##id', 'start': 24, 'end': 26}
{'entity': 'B-LOC', 'score': 0.99956733, 'index': 12, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>In this case the output of the pipeline is a list of dictionaries,
each one representing only entity <code>IOB</code> labels at the BERT
token level. IMPORTANT: this list is per wordPiece and NOT per <em>human
word</em> even if the provided text is pre-tokenized. You can assume all
of the tokens that don’t appear in the output were labeled as no-entity,
that is <code>"O"</code>. To recover the full-word entities you can
initialize the pipeline with
<code>aggregation_strategy="first"</code>:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, aggregation_strategy<span class="op">=</span><span class="st">"first"</span>)</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code now prints the following:</p>
<pre><code>{'entity_group': 'PER', 'score': 0.9995944, 'word': 'Wolfgang Schmid', 'start': 11, 'end': 26}
{'entity_group': 'LOC', 'score': 0.99956733, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>As you can see, entities aggregated at the Span Leven (instead of the
Token Level). Word pieces are merged back into <em>human words</em> and
also multiword entities are assigned a single entity label unifying the
<code>IOB</code> labels into one. Depending on your use case you can
request the pipeline to give different
<code>aggregation_strateg[ies]</code>. More info about the pipeline can
be found <a href="https://huggingface.co/docs/transformers/main_classes/pipelines" class="external-link">here</a>.</p>
<p>The next step is crucial: evaluate how does the pre-trained model
actually performs in <strong>your dataset</strong>. This is important
since the fine-tuned model could be overfitted to other custom
benchmarks that do not share the characteristics of your dataset.</p>
<p>To observe this, we can first see the performance on the test portion
of the dataset in which this classifier was trained, and then evaluate
the same pre-trained classifier on a NER dataset form a different
domain.</p>
</div>
</div>
<div class="section level1">
<h1 id="end-here--">———- END HERE ??? ———-<a class="anchor" aria-label="anchor" href="#end-here--"></a>
</h1>
<p>The rest is more advanced content (still I leave it here just in case
for now).</p>
<div class="section level2">
<h2 id="testing-on-conll-03-benchmark">Testing on CoNLL-03 Benchmark<a class="anchor" aria-label="anchor" href="#testing-on-conll-03-benchmark"></a>
</h2>
<p>This model was trained on the CoNLL-03 dataset, therefore we can
corroborate how it performs using the test portion of this dataset. To
get the data we can use the <code>datasets</code> library which is also
part of theHuggingFace landscape</p>
<pre><code>pip install datasets</code></pre>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>conll03_data <span class="op">=</span> load_dataset(<span class="st">"eriktks/conll2003"</span>, split<span class="op">=</span><span class="st">"test"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>conll03_data</span></code></pre>
</div>
<p>This shows the features and number of records of the CoNLL-03
Dataset. Next we can observe which labels we have in the data</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>conll03_data.features[<span class="st">'ner_tags'</span>]</span></code></pre>
</div>
<p>As expected, the labels are in IOB notation, where each label
corresponds to one word in the dataset, however the dataset contains the
labelIDs and we need to map them to their string representations. We can
double check this by looking at one of the records of the dataset:</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="kw">def</span> labelid2str(label_int):</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a>    d <span class="op">=</span> conll03_data.features[<span class="st">'ner_tags'</span>].feature._int2str</span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>    <span class="cf">return</span> d[label_int]</span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a>example_id <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a><span class="bu">print</span>(conll03_data[<span class="st">'tokens'</span>][example_id])</span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a><span class="bu">print</span>(conll03_data[<span class="st">'ner_tags'</span>][example_id])</span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a><span class="bu">print</span>([labelid2str(tag) <span class="cf">for</span> tag <span class="kw">in</span> conll03_data[<span class="st">'ner_tags'</span>][example_id]])</span></code></pre>
</div>
<p>These are the Gold Labels of the dataset. We can use our pre-trained
BERT model to predict the labels for each example and compare the
outputs to the gold labels provided in the data.</p>
<div class="section level3">
<h3 id="predictions-using-pipeline">Predictions using Pipeline<a class="anchor" aria-label="anchor" href="#predictions-using-pipeline"></a>
</h3>
<p>This could be done using the pipeline as we have been doing so far,
example by example:</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a></span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForTokenClassification</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a><span class="kw">def</span> get_gold_labels(label_ids):</span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a>    <span class="cf">return</span> [labelid2str(tag) <span class="cf">for</span> tag <span class="kw">in</span> label_ids]</span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a><span class="kw">def</span> token_to_spans(tokens):</span>
<span id="cb32-10"><a href="#cb32-10" tabindex="-1"></a>    token2spans <span class="op">=</span> {}</span>
<span id="cb32-11"><a href="#cb32-11" tabindex="-1"></a>    char_start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-12"><a href="#cb32-12" tabindex="-1"></a>    <span class="cf">for</span> i, tok <span class="kw">in</span> <span class="bu">enumerate</span>(tokens):</span>
<span id="cb32-13"><a href="#cb32-13" tabindex="-1"></a>        tok_end <span class="op">=</span> char_start <span class="op">+</span> <span class="bu">len</span>(tok)</span>
<span id="cb32-14"><a href="#cb32-14" tabindex="-1"></a>        token2spans[i] <span class="op">=</span> (char_start, tok_end)</span>
<span id="cb32-15"><a href="#cb32-15" tabindex="-1"></a>        char_start <span class="op">=</span> tok_end <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb32-16"><a href="#cb32-16" tabindex="-1"></a>    <span class="cf">return</span> token2spans</span>
<span id="cb32-17"><a href="#cb32-17" tabindex="-1"></a></span>
<span id="cb32-18"><a href="#cb32-18" tabindex="-1"></a><span class="kw">def</span> get_iob_from_aggregated(tokenized_sentence, entities):</span>
<span id="cb32-19"><a href="#cb32-19" tabindex="-1"></a>    <span class="co"># Initialize all labels empty</span></span>
<span id="cb32-20"><a href="#cb32-20" tabindex="-1"></a>    iob_labels <span class="op">=</span> [<span class="st">'O'</span>] <span class="op">*</span> <span class="bu">len</span>(tokenized_sentence)</span>
<span id="cb32-21"><a href="#cb32-21" tabindex="-1"></a>    <span class="co"># Get Token &lt;-&gt; Chars Mapping</span></span>
<span id="cb32-22"><a href="#cb32-22" tabindex="-1"></a>    tok2spans <span class="op">=</span> token_to_spans(tokenized_sentence)</span>
<span id="cb32-23"><a href="#cb32-23" tabindex="-1"></a>    start2tok <span class="op">=</span> {v[<span class="dv">0</span>]:k <span class="cf">for</span> k, v <span class="kw">in</span> tok2spans.items()}</span>
<span id="cb32-24"><a href="#cb32-24" tabindex="-1"></a>    end2tok <span class="op">=</span> {v[<span class="dv">1</span>]:k <span class="cf">for</span> k, v <span class="kw">in</span> tok2spans.items()}</span>
<span id="cb32-25"><a href="#cb32-25" tabindex="-1"></a>    <span class="co"># Iterate over each entity to populate labels</span></span>
<span id="cb32-26"><a href="#cb32-26" tabindex="-1"></a>    <span class="cf">for</span> entity <span class="kw">in</span> entities:</span>
<span id="cb32-27"><a href="#cb32-27" tabindex="-1"></a>        label <span class="op">=</span> entity[<span class="st">'entity_group'</span>]</span>
<span id="cb32-28"><a href="#cb32-28" tabindex="-1"></a>        token_start <span class="op">=</span> start2tok.get(entity[<span class="st">'start'</span>])</span>
<span id="cb32-29"><a href="#cb32-29" tabindex="-1"></a>        token_end <span class="op">=</span> end2tok.get(entity[<span class="st">'end'</span>])</span>
<span id="cb32-30"><a href="#cb32-30" tabindex="-1"></a>        </span>
<span id="cb32-31"><a href="#cb32-31" tabindex="-1"></a>        <span class="cf">if</span> token_start <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-32"><a href="#cb32-32" tabindex="-1"></a>            iob_labels[token_start] <span class="op">=</span> <span class="ss">f'B-</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb32-33"><a href="#cb32-33" tabindex="-1"></a>            <span class="cf">if</span> token_end <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-34"><a href="#cb32-34" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(token_start<span class="op">+</span><span class="dv">1</span>, token_end<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb32-35"><a href="#cb32-35" tabindex="-1"></a>                    iob_labels[i] <span class="op">=</span> <span class="ss">f'I-</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb32-36"><a href="#cb32-36" tabindex="-1"></a>    </span>
<span id="cb32-37"><a href="#cb32-37" tabindex="-1"></a>    <span class="cf">return</span> iob_labels</span>
<span id="cb32-38"><a href="#cb32-38" tabindex="-1"></a></span>
<span id="cb32-39"><a href="#cb32-39" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb32-40"><a href="#cb32-40" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb32-41"><a href="#cb32-41" tabindex="-1"></a></span>
<span id="cb32-42"><a href="#cb32-42" tabindex="-1"></a>example <span class="op">=</span> conll03_data[<span class="st">'tokens'</span>][example_id]</span>
<span id="cb32-43"><a href="#cb32-43" tabindex="-1"></a>example_str <span class="op">=</span> <span class="st">" "</span>.join(example)</span>
<span id="cb32-44"><a href="#cb32-44" tabindex="-1"></a></span>
<span id="cb32-45"><a href="#cb32-45" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"ner"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, aggregation_strategy<span class="op">=</span><span class="st">"first"</span>)</span>
<span id="cb32-46"><a href="#cb32-46" tabindex="-1"></a>predictions <span class="op">=</span> ner_classifier(example_str)</span>
<span id="cb32-47"><a href="#cb32-47" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SENTENCE:"</span>, example_str)</span>
<span id="cb32-48"><a href="#cb32-48" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PREDICTED:"</span>, get_iob_from_aggregated(example, predictions))</span>
<span id="cb32-49"><a href="#cb32-49" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GOLD:"</span>, get_gold_labels(conll03_data[<span class="st">'ner_tags'</span>][example_id]))</span></code></pre>
</div>
<p>Now that we understand how to get a list of Predicted labels for one
example we can run the model for the whole test data:</p>
<div class="codewrapper sourceCode" id="cb33">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a>all_predictions <span class="op">=</span> []</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a><span class="cf">for</span> example <span class="kw">in</span> conll03_data[<span class="st">'tokens'</span>]:</span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a>    output <span class="op">=</span> ner_classifier(<span class="st">" "</span>.join(example))</span>
<span id="cb33-4"><a href="#cb33-4" tabindex="-1"></a>    predictions <span class="op">=</span> get_iob_from_aggregated_simple(example, output)</span>
<span id="cb33-5"><a href="#cb33-5" tabindex="-1"></a>    all_predictions.append(predictions)</span>
<span id="cb33-6"><a href="#cb33-6" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" tabindex="-1"></a>gold_labels <span class="op">=</span> [get_gold_labels(lbl) <span class="cf">for</span> lbl <span class="kw">in</span> conll03_data[<span class="st">'ner_tags'</span>]]</span></code></pre>
</div>
<p>We can use the <code>seqeval</code> package to directly evaluate the
outputs:</p>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="im">from</span> seqeval.metrics <span class="im">import</span> classification_report</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>report <span class="op">=</span> classification_report(gold_labels, all_predictions)</span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a><span class="bu">print</span>(report)</span></code></pre>
</div>
<p>The three most basic metrics for NLP classifiers are traditionally
Precision, Recall and F1 score. They come from the Information
Extraction field and roughly they aim to measure the following: -
<strong>Precision (P):</strong> From the predicted entities, how many of
them are correct (i.e. match the gold labels)? - <strong>Recall
(R):</strong> From the known gold entities, how many of them were
predicted by the model? - <strong>F1 Score (F1):</strong> the harmonic
mean of precison and recall, which aims to provide a balance between
both metrics. It has two variants: the Micro-F1 which treats all errors
equally, being the same as measuring Accuracy; and Macro-F1, which aims
to show the model performance taking into account the label
distribution, this is normally the score reported through main
benchmarks as it shows better the model’s weaknesses across classes.</p>
</div>
</div>
<div class="section level2">
<h2 id="using-a-pre-trained-model-on-litbank">Using a Pre-trained Model on LitBank<a class="anchor" aria-label="anchor" href="#using-a-pre-trained-model-on-litbank"></a>
</h2>
<p>We can of course also use the pre-trained NER classifier with any
<strong>custom dataset</strong>, it will just need come pre- and
post-processing steps to make it work. For this example, we will use the
<a href="https://github.com/dbamman/litbank" class="external-link">LitBank</a> corpus, an
annotated dataset of 100 works of English-language fiction to support
tasks in natural language processing and the computational humanities.
Specifically they have human annotations of entities on these books. We
can measure how good is this pre-trained classifier by making the model
predict the entities inside the text and them compare the outputs with
the humam annotations. The NER portion of the dataset we will use is the
tabulated data from <a href="https://github.com/dbamman/litbank/tree/master/entities/tsv" class="external-link">here</a>
and one example looks like this:</p>
<table class="table">
<thead><tr class="header">
<th>Index</th>
<th>Token</th>
<th>IOB-1</th>
<th>IOB-2</th>
<th>IOB-3</th>
<th>IOB-4</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>CHAPTER</td>
<td>O</td>
<td>O</td>
<td>O</td>
<td>O</td>
</tr>
<tr class="even">
<td>2</td>
<td>I</td>
<td>O</td>
<td>O</td>
<td>O</td>
<td>O</td>
</tr>
<tr class="odd">
<td>3</td>
<td>In</td>
<td>O</td>
<td>O</td>
<td>O</td>
<td>O</td>
</tr>
<tr class="even">
<td>4</td>
<td>Chancery</td>
<td>B-FAC</td>
<td>O</td>
<td>O</td>
<td>O</td>
</tr>
<tr class="odd">
<td>5</td>
<td>London</td>
<td>B-GPE</td>
<td>O</td>
<td>O</td>
<td>O</td>
</tr>
<tr class="even">
<td>6</td>
<td>.</td>
<td>O</td>
<td>O</td>
<td>O</td>
<td>O</td>
</tr>
</tbody>
</table>
<p>It contains the information of 4 annotators, this is very useful
interannotator agreement, a technique in computational linguistics for
validating the correctness and consistency of the dataset. Yes! Humans
are wrong too all the time when labeling! For simplicity, we will assume
we only have the information from annotator 1 and take that as our
ground truth.</p>
<p>The format of the dataset resembles the conll format, a widely used
format in computational linguistics for token-based annotations. Another
important aspect to observe is that they have other labels for entities.
The pre-trained model we chose only labels PER, LOC, ORG and MISC. We
can translate FAC and GPE to LOC label as they are only more
fine-grained occurrences of locations which our model should recognize
as such. To read the data we can use the following function:</p>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a></span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a><span class="kw">def</span> quick_conll_reader(filepath):</span>
<span id="cb35-3"><a href="#cb35-3" tabindex="-1"></a>    all_sentences, all_labels <span class="op">=</span> [], []</span>
<span id="cb35-4"><a href="#cb35-4" tabindex="-1"></a>    sent_txt, sent_lbl <span class="op">=</span> [], []</span>
<span id="cb35-5"><a href="#cb35-5" tabindex="-1"></a>    label_vocab <span class="op">=</span> {}</span>
<span id="cb35-6"><a href="#cb35-6" tabindex="-1"></a>    gold_label_column <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb35-7"><a href="#cb35-7" tabindex="-1"></a>    label_translator <span class="op">=</span> {</span>
<span id="cb35-8"><a href="#cb35-8" tabindex="-1"></a>        <span class="st">"B-FAC"</span>: <span class="st">"O"</span>,</span>
<span id="cb35-9"><a href="#cb35-9" tabindex="-1"></a>        <span class="st">"I-FAC"</span>: <span class="st">"O"</span>,</span>
<span id="cb35-10"><a href="#cb35-10" tabindex="-1"></a>        <span class="st">"B-GPE"</span>: <span class="st">"B-LOC"</span>,</span>
<span id="cb35-11"><a href="#cb35-11" tabindex="-1"></a>        <span class="st">"I-GPE"</span>: <span class="st">"I-LOC"</span>,</span>
<span id="cb35-12"><a href="#cb35-12" tabindex="-1"></a>        <span class="st">"B-VEH"</span>: <span class="st">"O"</span>,</span>
<span id="cb35-13"><a href="#cb35-13" tabindex="-1"></a>        <span class="st">"I-VEH"</span>: <span class="st">"O"</span></span>
<span id="cb35-14"><a href="#cb35-14" tabindex="-1"></a>    }</span>
<span id="cb35-15"><a href="#cb35-15" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filepath) <span class="im">as</span> f:</span>
<span id="cb35-16"><a href="#cb35-16" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> f.readlines():</span>
<span id="cb35-17"><a href="#cb35-17" tabindex="-1"></a>            row <span class="op">=</span> line.strip().split(<span class="st">"</span><span class="ch">\t</span><span class="st">"</span>)</span>
<span id="cb35-18"><a href="#cb35-18" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(row) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb35-19"><a href="#cb35-19" tabindex="-1"></a>                sent_txt.append(row[<span class="dv">0</span>])</span>
<span id="cb35-20"><a href="#cb35-20" tabindex="-1"></a>                label <span class="op">=</span> row[gold_label_column]</span>
<span id="cb35-21"><a href="#cb35-21" tabindex="-1"></a>                <span class="cf">if</span> label <span class="kw">in</span> label_translator:</span>
<span id="cb35-22"><a href="#cb35-22" tabindex="-1"></a>                    final_label <span class="op">=</span> label_translator[label]</span>
<span id="cb35-23"><a href="#cb35-23" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb35-24"><a href="#cb35-24" tabindex="-1"></a>                    final_label <span class="op">=</span> label</span>
<span id="cb35-25"><a href="#cb35-25" tabindex="-1"></a>                sent_lbl.append(final_label)</span>
<span id="cb35-26"><a href="#cb35-26" tabindex="-1"></a>                <span class="cf">if</span> final_label <span class="kw">not</span> <span class="kw">in</span> label_vocab:</span>
<span id="cb35-27"><a href="#cb35-27" tabindex="-1"></a>                    label_vocab[final_label] <span class="op">=</span> <span class="bu">len</span>(label_vocab) </span>
<span id="cb35-28"><a href="#cb35-28" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb35-29"><a href="#cb35-29" tabindex="-1"></a>                all_sentences.append(<span class="st">" "</span>.join(sent_txt))</span>
<span id="cb35-30"><a href="#cb35-30" tabindex="-1"></a>                all_labels.append(sent_lbl)</span>
<span id="cb35-31"><a href="#cb35-31" tabindex="-1"></a>                sent_txt, sent_lbl <span class="op">=</span> [], []</span>
<span id="cb35-32"><a href="#cb35-32" tabindex="-1"></a>    <span class="cf">return</span> all_sentences, all_labels, label_vocab</span>
<span id="cb35-33"><a href="#cb35-33" tabindex="-1"></a></span>
<span id="cb35-34"><a href="#cb35-34" tabindex="-1"></a></span>
<span id="cb35-35"><a href="#cb35-35" tabindex="-1"></a>sentences, gold_labels, label_vocab <span class="op">=</span> quick_conll_reader(<span class="st">"1023_bleak_house_brat.tsv"</span>)</span>
<span id="cb35-36"><a href="#cb35-36" tabindex="-1"></a></span>
<span id="cb35-37"><a href="#cb35-37" tabindex="-1"></a><span class="bu">print</span>(sentences[<span class="dv">0</span>].split(<span class="st">' '</span>))</span>
<span id="cb35-38"><a href="#cb35-38" tabindex="-1"></a><span class="bu">print</span>(gold_labels[<span class="dv">0</span>])</span></code></pre>
</div>
<p>This code processes the <em>Bleak House</em> book and extracts a list
of tokenized sentences (as strings) and a list of IOB Labels
corresponding to each token in the sentence. You can see the first
sentence and its corresponding list of <em>gold labels</em> on this
example. Next, we load the NER pre-trained model again and process the
sentences to obtain model predictions. The problem here is that the
model predictions are lists of dictionaries and we need to post-process
them so they are also on IOB-format. We use the get_iob_labels()
function to do this conversion.</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a></span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a><span class="kw">def</span> token_to_spans(tokens):</span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a>    token2spans <span class="op">=</span> {}</span>
<span id="cb36-4"><a href="#cb36-4" tabindex="-1"></a>    char_start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-5"><a href="#cb36-5" tabindex="-1"></a>    <span class="cf">for</span> i, tok <span class="kw">in</span> <span class="bu">enumerate</span>(tokens):</span>
<span id="cb36-6"><a href="#cb36-6" tabindex="-1"></a>        tok_end <span class="op">=</span> char_start <span class="op">+</span> <span class="bu">len</span>(tok)</span>
<span id="cb36-7"><a href="#cb36-7" tabindex="-1"></a>        token2spans[i] <span class="op">=</span> (char_start, tok_end)</span>
<span id="cb36-8"><a href="#cb36-8" tabindex="-1"></a>        char_start <span class="op">=</span> tok_end <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb36-9"><a href="#cb36-9" tabindex="-1"></a>    <span class="cf">return</span> token2spans</span>
<span id="cb36-10"><a href="#cb36-10" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" tabindex="-1"></a><span class="kw">def</span> get_litbank_labels(tokenized_sentence, entities):</span>
<span id="cb36-13"><a href="#cb36-13" tabindex="-1"></a>    <span class="co"># Initialize all labels empty</span></span>
<span id="cb36-14"><a href="#cb36-14" tabindex="-1"></a>    iob_labels <span class="op">=</span> [<span class="st">'O'</span>] <span class="op">*</span> <span class="bu">len</span>(tokenized_sentence)</span>
<span id="cb36-15"><a href="#cb36-15" tabindex="-1"></a>    <span class="co"># Get Token &lt;-&gt; Chars Mapping</span></span>
<span id="cb36-16"><a href="#cb36-16" tabindex="-1"></a>    tok2spans <span class="op">=</span> token_to_spans(tokenized_sentence)</span>
<span id="cb36-17"><a href="#cb36-17" tabindex="-1"></a>    start2tok <span class="op">=</span> {v[<span class="dv">0</span>]:k <span class="cf">for</span> k, v <span class="kw">in</span> tok2spans.items()}</span>
<span id="cb36-18"><a href="#cb36-18" tabindex="-1"></a>    end2tok <span class="op">=</span> {v[<span class="dv">1</span>]:k <span class="cf">for</span> k, v <span class="kw">in</span> tok2spans.items()}</span>
<span id="cb36-19"><a href="#cb36-19" tabindex="-1"></a>    <span class="co"># Iterate over each entity to populate labels</span></span>
<span id="cb36-20"><a href="#cb36-20" tabindex="-1"></a>    <span class="cf">for</span> entity <span class="kw">in</span> entities:</span>
<span id="cb36-21"><a href="#cb36-21" tabindex="-1"></a>        label <span class="op">=</span> entity[<span class="st">'entity_group'</span>]</span>
<span id="cb36-22"><a href="#cb36-22" tabindex="-1"></a>        <span class="cf">if</span> label <span class="op">==</span> <span class="st">"MISC"</span>:  <span class="co"># Design choice: Do NOT count MISC entities!</span></span>
<span id="cb36-23"><a href="#cb36-23" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb36-24"><a href="#cb36-24" tabindex="-1"></a>        token_start <span class="op">=</span> start2tok.get(entity[<span class="st">'start'</span>])</span>
<span id="cb36-25"><a href="#cb36-25" tabindex="-1"></a>        token_end <span class="op">=</span> end2tok.get(entity[<span class="st">'end'</span>])</span>
<span id="cb36-26"><a href="#cb36-26" tabindex="-1"></a>        </span>
<span id="cb36-27"><a href="#cb36-27" tabindex="-1"></a>        <span class="cf">if</span> token_start <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb36-28"><a href="#cb36-28" tabindex="-1"></a>            iob_labels[token_start] <span class="op">=</span> <span class="ss">f'B-</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb36-29"><a href="#cb36-29" tabindex="-1"></a>            <span class="cf">if</span> token_end <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb36-30"><a href="#cb36-30" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(token_start<span class="op">+</span><span class="dv">1</span>, token_end<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb36-31"><a href="#cb36-31" tabindex="-1"></a>                    iob_labels[i] <span class="op">=</span> <span class="ss">f'I-</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb36-32"><a href="#cb36-32" tabindex="-1"></a>    </span>
<span id="cb36-33"><a href="#cb36-33" tabindex="-1"></a>    <span class="cf">return</span> iob_labels</span></code></pre>
</div>
<p>And we finally apply the model to the sentences that we previously
read:</p>
<div class="codewrapper sourceCode" id="cb37">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a></span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(sentences)</span>
<span id="cb37-3"><a href="#cb37-3" tabindex="-1"></a>model_predictions <span class="op">=</span> []</span>
<span id="cb37-4"><a href="#cb37-4" tabindex="-1"></a><span class="cf">for</span> i, sentence_ner <span class="kw">in</span> <span class="bu">enumerate</span>(ner_results):</span>
<span id="cb37-5"><a href="#cb37-5" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">===== SENTENCE </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> ====="</span>)</span>
<span id="cb37-6"><a href="#cb37-6" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Tokens:'</span>, sentences[i].split())</span>
<span id="cb37-7"><a href="#cb37-7" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'GOLD:'</span>, gold_labels[i])</span>
<span id="cb37-8"><a href="#cb37-8" tabindex="-1"></a>    <span class="co"># Get the IOB labels for the tokenized sentence</span></span>
<span id="cb37-9"><a href="#cb37-9" tabindex="-1"></a>    tokenized_sentence <span class="op">=</span> sentences[i].split()</span>
<span id="cb37-10"><a href="#cb37-10" tabindex="-1"></a>    predicted_iob_labels <span class="op">=</span> get_litbank_labels(tokenized_sentence, sentence_ner)</span>
<span id="cb37-11"><a href="#cb37-11" tabindex="-1"></a>    model_predictions.append(predicted_iob_labels)</span>
<span id="cb37-12"><a href="#cb37-12" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'MODEL:'</span>, predicted_iob_labels)</span>
<span id="cb37-13"><a href="#cb37-13" tabindex="-1"></a>    <span class="cf">for</span> nr <span class="kw">in</span> sentence_ner:</span>
<span id="cb37-14"><a href="#cb37-14" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\t</span><span class="sc">{</span>nr<span class="sc">}</span><span class="ss">'</span>)</span></code></pre>
</div>
<p>For each model prediction we are printing the sentence tokens, the
IOB gold labels and the IOB predicitons. Now that the data is in this
shape we can perform evaluation.</p>
</div>
<div class="section level2">
<h2 id="model-evaluation">Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"></a>
</h2>
<p>To perform evaluation in your data you can use again the
<code>seqeval</code> package:</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a></span>
<span id="cb38-2"><a href="#cb38-2" tabindex="-1"></a><span class="im">from</span> seqeval.metrics <span class="im">import</span> classification_report</span>
<span id="cb38-3"><a href="#cb38-3" tabindex="-1"></a><span class="bu">print</span>(classification_report(gold_labels, model_predictions))</span></code></pre>
</div>
<p>Since we took a classifier that was not trained for the book domain,
the performance is quite poor. But this example shows us that
classifiers performing very well on their own domain most of the times
transfer poorly to other apparently similar datasets.</p>
<p>The solution in this case is to use another of the great
characteristics of BERT: fine-tuning for domain adaptation. It is
possible to train your own classifier with relatively small data (given
that a lot of linguistic knowledge was already provided during the
language modeling pre-training). In the following section we will see
how to train your own NER model and use it for predictions.</p>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.10" class="external-link">sandpaper (0.16.10)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.7" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/aio.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/aio.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-01-09",
  "datePublished": "2025-01-09"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

