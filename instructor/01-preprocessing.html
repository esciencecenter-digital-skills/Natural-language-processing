<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>fundamentals of Natural Language Processing (NLP) in Python: Episode 1: From text to vectors</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png"><link rel="manifest" href="../favicons/incubator/site.webmanifest"><link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../01-preprocessing.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 20%" class="percentage">
    20%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 20%" aria-valuenow="20" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../01-preprocessing.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="00-introduction.html">1. Introduction</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        2. Episode 1: From text to vectors
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#introduction">Introduction</a></li>
<li><a href="#formulate-the-problem">1. Formulate the problem</a></li>
<li><a href="#semantic-shift">Semantic shift</a></li>
<li><a href="#download-the-data">2. Download the data</a></li>
<li><a href="#inspect-the-data">Inspect the data</a></li>
<li><a href="#prepare-data-to-be-ingested-by-the-model-preprocessing">3. Prepare data to be ingested by the model (preprocessing)</a></li>
<li><a href="#tracing-semantic-shifts-with-word-embeddings">Tracing semantic shifts with word embeddings</a></li>
<li><a href="#what-are-word-embeddings">What are word embeddings?</a></li>
<li><a href="#train-the-word2vec-model">4. Train the Word2Vec model</a></li>
<li><a href="#load-the-embeddings-and-inspect-them">5. Load the embeddings and inspect them</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="02-transformers.html">3. Episode 2: BERT and Transformers</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr></ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/00-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/02-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/00-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction
        </a>
        <a class="chapter-link float-end" href="../instructor/02-transformers.html" rel="next">
          Next: Episode 2: BERT and...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Episode 1: From text to vectors</h1>
        <p>Last updated on 2024-12-31 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-preprocessing.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 230 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>Why do we need to prepare a text for training?</li>
<li>How do I prepare a text to be used as input to a model?</li>
<li>What different types of pre processing steps are there?</li>
<li>How do I train a neural network to extract word embeddings?</li>
<li>What properties word embeddings have?</li>
<li>What is a word2vec model?</li>
<li>How do we train a word2vec model?</li>
<li>How do I get insights regarding my text, based on the word
embeddings?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul><li>Implement a full preprocessing pipeline on a text</li>
<li>Use Word2Vec to train a model</li>
<li>Inspect word embeddings</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a></h2>
<hr class="half-width"><p>In this episode, we’ll train a neural network to obtain word
embeddings. We will only briefly touch upon the concepts of
<code>preprocessing</code> and <code>word embedding</code> with
Word2vec.</p>
<p>The idea is to get you over a practical example first, without diving
into the technical or mathematical intricacies of neural networks and
word embeddings. The goal for this episode, in fact, is for you to get
an intuition of how computers represent language. This is key to
understand how NLP applications work and what are their limits.</p>
<p>In the later episodes we will build upon this knowledge to go deeper
into all of these concepts and see how NLP tools have evolved more
complex language representations.</p>
<p>In this episode, we will build a workflow following these steps:</p>
<ol style="list-style-type: decimal"><li>Formulate the problem</li>
<li>Download the input data</li>
<li>Prepare data to be ingested by the model (i.e. preprocessing
step)</li>
<li>Train the model</li>
<li>Load the embeddings and inspect them</li>
</ol><p>Note that for step 5 we will cover only briefly the code to train
your own model, but then we will load the output of already pretrained
models. That is because training requires a large amount of data and
considerable computing resources/time which are not suitable for a local
laptop/computer.</p>
</section><section><h2 class="section-heading" id="formulate-the-problem">1. Formulate the problem<a class="anchor" aria-label="anchor" href="#formulate-the-problem"></a></h2>
<hr class="half-width"><p>In this episode we will be using Dutch newspaper texts to train a
Word2Vec model to investigate the notion of <em>semantic shift</em>.</p>
</section><section><h2 class="section-heading" id="semantic-shift">Semantic shift<a class="anchor" aria-label="anchor" href="#semantic-shift"></a></h2>
<hr class="half-width"><p>Semantic shift, as it is used here, refers to a pair of meanings A
and B which are linked by some relation. Either diachronically (e.g.,
Latin <em>caput</em> “head” and Italian <em>capo</em> “chief”) or
synchronically, e.g. as two meanings that co-exist in a word
simoultaneously (English “head”, as in “I have covered my head with a
hat” and as in “I am the head of the department”). <strong>Can we detect
a semantic shift?</strong> – We’ll tackle this phenomenon in this
episode.</p>
<p>Newspapers make an interesting dataset for investigating this
phenomenon, as they contain information about current events and the
language it uses is clear and reflective of its time. We will
specifically look at the evolution of specific words in Dutch across a
period of time from 1950 to 1990. In order to do that, we need to train
a model to extract the meaning of every single word and track in which
context they occur, over decades.</p>
<div id="goal" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="goal" class="callout-inner">
<h3 class="callout-title">Goal</h3>
<div class="callout-content">
<p>The goal is to analyze the semantic shift of specific Dutch words
from 1950 to 1989 using newspapers as a dataset.</p>
</div>
</div>
</div>
<p>Conceptually, the task of discovery of semantic shifts in our
newspaper data can be formulated as follows:</p>
<p>Given newspaper corpora [C1, C2, C3, …] containing texts created in
time periods from 1950s to 1980s, considered as four decades [1: 50-60;
2: 60-70; 3: 70-80; 4: 80-90], the task is to detect how some words have
shifted in meaning across those decades.</p>
<p>As a test-bed, we’re going to focus on three words:
<code>mobiel</code>, <code>televisie</code> and <code>ijzeren</code>.
These words exemplify very well the notion of semantic evolution /
semantic shift, as their meaning has gained new nuances due to social,
technological, political and economic changes occurred in those key
years.</p>
<p>We’re going to use a model to solve this task. We’re going to see
which one and how in a moment.</p>
<p>Our dataset is provided by Delpher (developed by the <a href="https://www.kb.nl/" class="external-link">KB - the National Library of the
Netherlands</a>) which contains digitalised historic Dutch newspapers,
books, and magazines. This online newspaper collection covers data
spanning from 1618 up to 1995 and of many local, national and
international publishers.</p>
<p>We will load only a page to go step-by-step through what it takes to
train a model. This makes it easier to know what’s going on. However, in
practice, when to successfully train a model you need larger quantities
of data to allow the model to get more precise and accurate
representations. In those cases you will simply condense each of the
steps we cover next into one code, to do all these steps at once.</p>
<div id="dataset-size-in-training" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="dataset-size-in-training" class="callout-inner">
<h3 class="callout-title">Dataset size in training</h3>
<div class="callout-content">
<p>To obtain high-quality embeddings, the size/length of your training
dataset plays a crucial role. Generally <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" class="external-link">tens of
thousands of documents</a> are considered a reasonable amount of data
for decent results.</p>
<p>Is there however a strict minimum? Not really. Things to keep in mind
is that <code>vocabulary size</code>, <code>document length</code> and
<code>desired vector size</code> interacts with each other. The higher
the dimensional vectors (e.g. 200-300 dimensions) the more data is
required, and of high quality, i.e. that allows the learning of words in
a variety of contexts.</p>
<p>While word2vec models typically perform better with large datasets
containing millions of words, using a single page is sufficient for
demonstration and learning purposes. This smaller dataset allows us to
train the model quickly and understand how word2vec works without the
need for extensive computational resources.</p>
</div>
</div>
</div>
<p>For the purpose of this episode and to make training easy on our
laptop, we’ll train our word2vec model using <strong>just one
page</strong>. Subsequently, we’ll load pre-trained models for tackling
our task.</p>
<div id="exploring-delpher" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exploring-delpher" class="callout-inner">
<h3 class="callout-title">Exploring Delpher</h3>
<div class="callout-content">
<p>Before we move further with our problem, take your time to explore
Delpher more in detail. Go to <a href="https://www.delpher.nl/" class="external-link">Delpher</a> and pick a newspaper of a
particular date. Did you find anything in the newspaper that is
interesting or didn’t know yet? For example about your living area,
sports club, or an historic event?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Few examples. </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li><p>The 20th of July 1969 marks an important event. The First Moon
landing! Look at what the <a href="https://resolver.kb.nl/resolve?urn=KBPERS01:003319021:mpeg21:pdf" class="external-link">Tubantia
newspaper</a> had to say about it only four days afterwards.</p></li>
<li><p>The Cuban Missile Crisis, also known as the October Crisis in
Cuba, or the Caribbean Crisis, was a 13-day confrontation between the
governments of the United States and the Soviet Union, when American
deployments of nuclear missiles in Italy and Turkey were matched by
Soviet deployments of nuclear missiles in Cuba. The crisis lasted from
16 to 28 October 1962. See what de Volkskrant published on the <a href="https://resolver.kb.nl/resolve?urn=ABCDDD:010876534:mpeg21:pdf" class="external-link">24th
of October, 1962</a>. Can you see what they have organised in Den Haag
related to this event?</p></li>
</ol></div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="download-the-data">2. Download the data<a class="anchor" aria-label="anchor" href="#download-the-data"></a></h2>
<hr class="half-width"><p>We download a page from the journal <a href="https://www.delpher.nl/nl/kranten/view?coll=ddd&amp;query=&amp;cql%5B%5D=%28date+_gte_+%2220-07-1969%22%29&amp;redirect=true&amp;sortfield=date&amp;resultscoll=dddtitel&amp;identifier=KBPERS01:002846018:mpeg21&amp;rowid=3" class="external-link">Algemeen
Dagblad</a> of July 21, 1969 as <code>txt</code> and save it as
<code>ad.txt</code>. We then load this file and store it in a variable
called <code>corpus</code>.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>path <span class="op">=</span> <span class="st">"episodes/data/ad.txt"</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path) <span class="im">as</span> myfile:</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>    corpus <span class="op">=</span> myfile.read()</span></code></pre>
</div>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>The <code>txt</code> file provides the text without formatting and
images, and is the product of a technique called Optical Character
Recognition (OCR). This is a technique in which text from an image is
converted into text, and it’s a necessary step for any scanned image to
obtain plain text. Luckily for us, Delpher has already done this step
for us so that we can directly use the txt. However, take into
consideration that if you start from an image that contains text, you
may need an additional preprocessing step.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="inspect-the-data">Inspect the data<a class="anchor" aria-label="anchor" href="#inspect-the-data"></a></h2>
<hr class="half-width"><p>We inspect the first line of the imported text:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>corpus[:<span class="dv">100</span>]</span></code></pre>
</div>
<p><code>'MENS OP MAAN\n„De Eagle is geland” Reisduur: 102 uur, Uitstappen binnen 20 iuli, 21.17 uur 45 min. en'</code></p>
<p>We can see that although the OCR applied to the original image has
given a pretty good result, there are mistakes in the recognized text.
For example, on the first line the word <code>juli</code> (july) has
misinterpreted as <code>iuli</code>.</p>
<p>Note also the size of the text:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="bu">len</span>(corpus)</span></code></pre>
</div>
<p>There are <code>12354</code> characters inside the corpus. Note also
the type of file:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">type</span>(corpus)</span></code></pre>
</div>
<p>Python tells us that <code>corpus</code> is a <code>str</code>,
i.e. a string. This means that every single character in the text (even
blank spaces) is a unit for our computer. However, what’s really
important for us is that the machine gets the meaning of the
<strong>words</strong> contained in the text. That is, that is able to
understand which characters belong together to form a word, and what
instead is something else: Punctuation, conjunctions, articles, or
prepositions.</p>
<p>How do we teach our machine to <em>segment</em> the text and keep
only the relevant words? This is where <code>data preprocessing</code>
comes into play. It prepares the text for efficient processing by the
model, allowing it to focus on the important parts of the text that
contribute to understanding its meaning.</p>
</section><section><h2 class="section-heading" id="prepare-data-to-be-ingested-by-the-model-preprocessing">3. Prepare data to be ingested by the model (preprocessing)<a class="anchor" aria-label="anchor" href="#prepare-data-to-be-ingested-by-the-model-preprocessing"></a></h2>
<hr class="half-width"><p>NLP models work by learning the statistical regularities within the
constituent parts of the language (i.e, letters, digits, words and
sentences) in a text. However, text contains also other type of
information that humans find useful to convey meaning. To signal pauses,
give emphasis and convey tone, for instance, we use punctuation.
Articles, conjunctions and prepositions also alter the meaning of a
sentence. The machine does not know the difference among all of these
linguistic units, as it treats them all as equal. Also, the decision to
remove or retain these parts of text is quite crucial for training our
model, as it affects the quality of generated word vectors.</p>
<p>Examples of preprocessing steps are:</p>
<ul><li>Cleaning the text: remove symbols/special characters, or other
things that “sneaked” into the text while loading the original
version.</li>
<li>Lowercasing</li>
<li>Removing punctuation</li>
<li>Stop word removal, where you remove prepositions, conjuctions and
articles</li>
<li>Tokenization: this means segmenting the text by retaining groups of
characters. These groups are referred to as <code>tokens</code> and
their size can vary from entire words to lemmas, or subword components
(e.g. morphemes)</li>
<li>Part of speech tagging: the process of labelling the grammatical
role of a word, e.g. nouns and verbs.</li>
</ul><div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<ul><li><p>Preprocessing approaches affect significantly the quality of the
training when working with word embeddings. For example, [Rahimi &amp;
Homayounpour (2022)] (<a href="https://link.springer.com/article/10.1007/s10579-022-09620-5" class="external-link uri">https://link.springer.com/article/10.1007/s10579-022-09620-5</a>)
demonstrated that for text classification and sentiment analysis, the
removal of punctuation and stopwords leads to higher
performance.</p></li>
<li><p>You do not always need to do all the preprocessing steps, and
which ones you should do depends on what you want to do. For example, if
you want to extract entities from the text using named entity
recognition, you explicitly do not want to lowercase the text, as
capitals are a component in the identification process.</p></li>
<li><p>Preprocessing can be very diffent for different languages. This
is both in terms of which steps to apply, but also which methods to use
for a specific step.</p></li>
</ul></div>
</div>
</div>
<p>Let’s apply a number of preprocessing steps to extract a list of
words from the newspaper page.</p>
<div class="section level4">
<h4 id="cleaning-the-text">1. Cleaning the text<a class="anchor" aria-label="anchor" href="#cleaning-the-text"></a></h4>
<p>We start by importing the <code>spaCy</code> library that will help
us go through the preprocessing steps. SpaCy is a popular open-source
library for NLP in Python and it works with pre-trained languages models
that we can load and use to process and analyse the text
efficiently.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> spacy</span></code></pre>
</div>
<p>We need to install <code>en_core_web_sm</code> because the text we’re
dealing with it’s in Dutch This is a small pre-trained language <a href="https://spacy.io/models/nl/" class="external-link">model from Spacy</a> containing
essential components like vocabulary, syntax, and entities specifically
for the Dutch language.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>python <span class="op">-</span>m spacy download nl_core_news_sm</span></code></pre>
</div>
<p>We can then load the model into the pipeline function. This function
connects the pretrained model to various preprocessing steps, including
the tokenisation.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(corpus)</span></code></pre>
</div>
<p>Next, we’ll eliminate the triple dashes that separate different news
articles, as well as the vertical bars used to divide some columns.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># filter out triple dashes and vertical bars</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>filtered_tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.text <span class="op">!=</span> <span class="st">"---"</span> <span class="kw">and</span> token.text <span class="op">!=</span> <span class="st">"|"</span>]</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co"># join units back into a cleaned string</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>corpus_clean <span class="op">=</span> <span class="st">' '</span>.join(filtered_tokens)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="bu">print</span>(corpus_clean[:<span class="dv">100</span>])</span></code></pre>
</div>
<p><code>MENS OP MAAN „ De Eagle is geland ” Reisduur : 102 uur , Uitstappen binnen 20 iuli , 21.17 uur 45</code></p>
</div>
<div class="section level4">
<h4 id="lowercasing">2. Lowercasing<a class="anchor" aria-label="anchor" href="#lowercasing"></a></h4>
<p>Our next step is to lowercase the text. Our goal here is to generate
a list of unique words from the text, so in order to not have words
twice in the list - once normal and once capitalised when it is at the
start of a sentence for example - we can lowercase the full text.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>corpus_lower <span class="op">=</span> corpus_clean.lower()</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="bu">print</span>(corpus_lower)</span></code></pre>
</div>
<p><code>mens op maan \n „ de eagle is geland ” reisduur : 102 uur , uitstappen binnen 20 iuli , 21.17 uur 45 [...]</code></p>
<div id="callout5" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>It is important to keep in mind that in doing this, some information
is lost. As mentioned before, models that are trained to identify named
entities use information on capitalisation. As another example, there
are a lot of names and surnames that carry meaning. “Bakker” is a common
Dutch surname, but is also a noun (baker). In lowercasing the text you
loose the distinction between the two.</p>
</div>
</div>
</div>
<p>Next we move to tokenise our text.</p>
</div>
<div class="section level4">
<h4 id="tokenisation">4. Tokenisation<a class="anchor" aria-label="anchor" href="#tokenisation"></a></h4>
<p>Tokenisation is essential in NLP, as it helps to create structure
from raw text. It involves the segmentation of the text into smaller
units referred as <code>tokens</code>. Tokens can be sentences
(e.g. <code>'the happy cat'</code>), words
(<code>'the', 'happy', 'cat'</code>), subwords
(<code>'un', 'happiness'</code>) or characters
(<code>'c','a', 't'</code>). The choice of tokens depends by the
requirement of the model used for training, and the text. This step is
carried out by a pre-trained model (called tokeniser) that has been
fine-tuned for the target language. In our case, this is
<code>en_core_web_sm</code> loaded before.</p>
<div id="callout6" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>A good word tokeniser for example, does not simply break up a text
based on spaces and punctuation, but it should be able to
distinguish:</p>
<ul><li>abbreviations that include points (e.g.: <em>e.g.</em>)</li>
<li>times (<em>11:15</em>) and dates written in various formats
(<em>01/01/2024</em> or <em>01-01-2024</em>)</li>
<li>word contractions such as <em>don’t</em>, these should be split into
<em>do</em> and <em>n’t</em>
</li>
<li>URLs</li>
</ul><p>Many older tokenisers are rule-based, meaning that they iterate over
a number of predefined rules to split the text into tokens, which is
useful for splitting text into word tokens for example. Modern large
language models use subword tokenisation, which are more flexible.</p>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>spacy_corpus <span class="op">=</span> nlp(corpus_clean)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co"># Get the tokens from the pipeline</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> spacy_corpus]</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>tokens[:<span class="dv">10</span>]</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', '\n ', '„', 'de', 'eagle', 'is', 'geland', '”']</code></p>
<p>As one can see the tokeniser has split each word in a token, however
it has considered also blank spaces <code>\n</code> and also
punctuation.</p>
</div>
<div class="section level4">
<h4 id="remove-punctuation">5. Remove punctuation<a class="anchor" aria-label="anchor" href="#remove-punctuation"></a></h4>
<p>The next step we will apply is to remove punctuation. We are
interested in training our model to learn the meaning of the words. This
task is highly influenced by the state of our text and punctuation would
decrease the quality of the learning as it would add spurious
information. We’ll see how the learning process works later in the
episode.</p>
<p>The punctuation symbols are defined in:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>string.punctuation</span></code></pre>
</div>
<p>We can loop over these symbols to remove them from the text:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># remove punctuation from set</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> string.punctuation]</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co"># remove also blank spaces</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens_no_punct <span class="cf">if</span> token.strip() <span class="op">!=</span> <span class="st">''</span>]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="bu">print</span>(tokens_no_punct[:<span class="dv">10</span>])</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', 'de', 'eagle', 'is', 'geland', 'reisduur', '102', 'uur']</code></p>
</div>
<div class="section level4">
<h4 id="visualise-the-tokens">Visualise the tokens<a class="anchor" aria-label="anchor" href="#visualise-the-tokens"></a></h4>
<p>This was the end of our preprocessing step. Let’s look at what tokens
we have extracted and how frequently they occur in the text.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="co"># count the frequency of occurrence of each token</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>token_counts <span class="op">=</span> Counter(tokens_no_punct)</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a><span class="co"># get the top n most common tokens (otherwise the plot would be too crowded) and their relative frequencies</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>most_common <span class="op">=</span> token_counts.most_common(<span class="dv">100</span>)</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>tokens <span class="op">=</span> [item[<span class="dv">0</span>] <span class="cf">for</span> item <span class="kw">in</span> most_common]</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>frequencies <span class="op">=</span> [item[<span class="dv">1</span>] <span class="cf">for</span> item <span class="kw">in</span> most_common]</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>plt.bar(tokens, frequencies)</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>plt.xlabel(<span class="st">'Tokens'</span>)</span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a>plt.title(<span class="st">'Token Frequencies'</span>)</span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<p>As one can see, words in the text have a very specific <a href="https://link.springer.com/article/10.3758/s13423-014-0585-6" class="external-link">skewed
distribution</a>, such that there are few very high-frequency words that
account for most of the tokens in text (e.g., articles, conjunctions)
and many low frequency words.</p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Discuss with each other:</p>
<ul><li>For which NLP tasks can punctuation removal be applied?</li>
<li>For which tasks is punctuation relevant and should punctuation not
be removed?</li>
</ul></div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<ul><li>Laura left the solution here missing – could this be considered an
exercise without a solution?</li>
</ul></div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="stop-word-removal">6. Stop word removal<a class="anchor" aria-label="anchor" href="#stop-word-removal"></a></h4>
<p>For some NLP tasks only the important words in the text are needed. A
text however often contains many <code>stop words</code>: common words
such as <code>de</code>, <code>het</code>, <code>een</code> that add
little meaningful content compared to nouns and words. In those cases,
it is best to remove stop words from your corpus to reduce the number of
words to process.</p>
<div id="tasks-where-stop-word-removal-is-useful" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="tasks-where-stop-word-removal-is-useful" class="callout-inner">
<h3 class="callout-title">Tasks where stop word removal is useful</h3>
<div class="callout-content">
<p>NLP tasks for which stop word removal can be applied are for example
<code>text classification</code> or <code>topic modelling</code>. When
clustering words into topics, stop words are irrelevant. Having fewer
and more relevant words gives better results. For other tasks, such as
<code>text generation</code> or <code>question answering</code>, the
full structure and context are important, so stop words should
<em>not</em> be removed. This is also the case for
<code>named entity recognition</code>, since named entities can contain
stop words themselves.</p>
</div>
</div>
</div>
<p>The Dutch spaCy model contains a list of stop words in the Dutch
language.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>stopwords <span class="op">=</span> nlp.Defaults.stop_words</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(stopwords)[:<span class="dv">20</span>])</span></code></pre>
</div>
<p><code>['bijvoorbeeld', 'ikzelf', 'anderzijds', 'toch', 'jouwe', 'omtrent', 'geleden', 'een', 'met', 'voorts', 'pas', 'zal', 'meer', 'maar', 'wier', 'hen', 'hare', 'vervolgens', 'klaar', 'worden']</code></p>
<p>We proceed to remove it:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="co"># remove stopwords</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>tokens_no_stopwords <span class="op">=</span> tokens_no_punct</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a><span class="cf">for</span> stopword <span class="kw">in</span> stopwords:</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    tokens_no_stopwords <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens_no_stopwords <span class="cf">if</span> token <span class="op">!=</span> stopword]</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a><span class="bu">print</span>(tokens_no_stopwords[:<span class="dv">20</span>])</span></code></pre>
</div>
<p><code>['mens', 'maan', 'eagle', 'geland', 'reisduur', '102', 'uur', 'uitstappen', '20', 'iuli', '21.17', 'uur', '45', 'min.', '40', 'sec.', 'vijf', 'uur', 'landing', 'armstrong']</code></p>
</div>
<div class="section level4">
<h4 id="visualise-tokens-into-a-word-cloud">Visualise tokens into a word cloud<a class="anchor" aria-label="anchor" href="#visualise-tokens-into-a-word-cloud"></a></h4>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">from</span> wordcloud <span class="im">import</span> WordCloud</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>wordcloud <span class="op">=</span> WordCloud().generate(<span class="st">' '</span>.join(tokens_no_stopwords))</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>plt.imshow(wordcloud, interpolation<span class="op">=</span><span class="st">'bilinear'</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul><li>Preprocessing involves a number of steps that one can apply to their
text to prepare it for further processing.</li>
<li>Preprocessing is important because it can improve your results</li>
<li>You do not always need to do all preprocessing steps. It depends on
the task at hand which preprocessing steps are important.</li>
</ul></div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="tracing-semantic-shifts-with-word-embeddings">Tracing semantic shifts with word embeddings<a class="anchor" aria-label="anchor" href="#tracing-semantic-shifts-with-word-embeddings"></a></h2>
<hr class="half-width"><p>Now we will train a model to to detect how the meaning of
<code>ijzeren</code>, <code>televisie</code> and <code>mobiel</code>
have shifted over the years, from the 50s to the 80s. This model will
return us <code>distributional word representations</code>, also known
as <code>embeddings</code>.</p>
<p>A number of publications (e.g., <a href="https://www.jair.org/index.php/jair/article/view/10640" class="external-link">Turney et
al., 2010</a>; <a href="https://aclanthology.org/P14-1023.pdf" class="external-link">Baroni et
al., 2014</a>) have showed that embeddings provide an efficient way to
track how meanings of words change across years. Let’s see what are
those and how they manage to do that.</p>
</section><section><h2 class="section-heading" id="what-are-word-embeddings">What are word embeddings?<a class="anchor" aria-label="anchor" href="#what-are-word-embeddings"></a></h2>
<hr class="half-width"><p>A Word Embedding is a word representation type that maps words in a
numerical manner (i.e., into vectors) in a multidimensional space,
capturing their meaning based on characteristics or context. Since
similar words occur in similar contexts, or have same characteristics,
the system naturally learns to assign similar vectors to similar
words.</p>
<p>Let’s illustrate this concept using animals. This example will show
us an intuitive way of representing things into vectors.</p>
<p>Suppose we want to represent a <code>cat</code> using measurable
characteristics:</p>
<ul><li>Furriness: Let’s assign a score of 70 to a cat</li>
<li>Number of legs: A cat has 4 legs</li>
</ul><div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>cat <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">4</span>]])</span></code></pre>
</div>
<p>So the vector representation of a cat becomes:
<code>[70 (furriness), 4 (legs)]</code></p>
<figure><img src="../fig/emb3.png" class="figure mx-auto d-block"><div class="figcaption">Embedding of a cat - We have described it along
two dimensions: furriness and number of legs</div>
</figure><p>This vector doesn’t fully describe a cat but provides a basis for
comparison with other animals.</p>
<p>Let’s add vectors for a dog and a caterpillar:</p>
<ul><li>Dog: [56, 4]</li>
<li>Caterpillar: [70, 100]</li>
</ul><div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>dog <span class="op">=</span> np.array([[<span class="dv">56</span>, <span class="dv">4</span>]])</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">100</span>]])</span></code></pre>
</div>
<figure><img src="../fig/emb5.png" class="figure mx-auto d-block"><div class="figcaption">Embeddings of a cat and a dog and a
caterpillar</div>
</figure><p>To determine which animal is more similar to a cat, we use
<code>cosine similarity</code>, which measures the cosine of the angle
between two vectors.</p>
<div id="callout8" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p><a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> ranges between [<code>-1</code> and <code>1</code>]. It
is the cosine of the angle between two vectors, divided by the product
of their length. It is a useful metric to measure how similar two
vectors are likely to be.</p>
<figure><img src="../fig/emb12.png" alt="" class="figure mx-auto d-block"></figure></div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>similarity_cat_dog <span class="op">=</span> cosine_similarity(cat, dog)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>similarity_cat_caterpillar <span class="op">=</span> cosine_similarity(cat, caterpillar)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and dog: </span><span class="sc">{</span>similarity_cat_dog<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and caterpillar: </span><span class="sc">{</span>similarity_cat_caterpillar<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> dog: <span class="fl">0.9998987965747193</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> caterpillar: <span class="fl">0.6192653797321375</span></span></code></pre>
</div>
<p>The higher similarity score between the cat and the dog indicates
they are more similar based on these characteristics. Adding more
characteristics can enrich our vectors, detecting more semantic
nuances.</p>
<figure><img src="../fig/emb6.png" class="figure mx-auto d-block"><div class="figcaption">Embeddings of a cat and a dog and a caterpillar
- We can describe these animals in many dimensions!</div>
</figure><div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ul><li>Add one of two other dimensions. What characteristics could they
map?</li>
<li>Add another animal and map their dimensions</li>
<li>Compute again the cosine similarity among those animals and find the
couple that is the least similar and the most similar</li>
</ul></div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>Add one of two other dimensions</li>
</ol><p>We could add the dimension of “velocity” or “speed” that goes from 0
to 100 meters/second.</p>
<ul><li>Caterpillar: 0.001 m/s</li>
<li>Cat: 1.5 m/s</li>
<li>Dog: 2.5 m/s</li>
</ul><p>(just as an example, actual speeds may vary)</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>]])</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>]])</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>]])</span></code></pre>
</div>
<p>Another dimension could be weight in Kg:</p>
<ul><li>Caterpillar: .05 Kg</li>
<li>Cat: 4 Kg</li>
<li>Dog: 15 Kg</li>
</ul><p>(just as an example, actual weight may vary)</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>, <span class="dv">4</span>]])</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>, <span class="dv">15</span>]])</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>, <span class="fl">.05</span>]])</span></code></pre>
</div>
<p>Then the cosine similarity would be:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>cosine_similarity(cat, caterpillar)</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>cosine_similarity(cat, dog)</span></code></pre>
</div>
<p>Output:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>array([[<span class="fl">0.61814254</span>]])</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>array([[<span class="fl">0.97893809</span>]])</span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal"><li>Add another animal and map their dimensions</li>
</ol><p>Another animal that we could add is the Tarantula!</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>, <span class="dv">4</span>]])</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>, <span class="dv">15</span>]])</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>, <span class="fl">.05</span>]])</span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>tarantula <span class="op">=</span> np.asarray([[<span class="dv">80</span>, <span class="dv">6</span>, <span class="fl">.1</span>, <span class="fl">.3</span>]])</span></code></pre>
</div>
<ol start="3" style="list-style-type: decimal"><li>Compute again the cosine similarity among those animals - find out
the most and least similar couple</li>
</ol><p>Given the values above, the least similar couple is the dog and the
caterpillar, whose cosine similarity is
<code>array([[0.60855407]])</code>.</p>
<p>The most similar couple is the cat and the tarantula:
<code>array([[0.99822302]])</code></p>
</div>
</div>
</div>
</div>
<p>By representing words as vectors with multiple dimensions, we capture
more nuances of their meanings or characteristics.</p>
<div id="keypoints2" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul><li>We can represent text as vectors of numbers (which makes it
interpretable for machines)</li>
<li>The most efficient and useful way is to use word embeddings</li>
<li>We can easily compute how words are similar to each other with the
cosine similarity</li>
</ul></div>
</div>
</div>
<p>When semantic change occurs, words in their context <em>also</em>
change. We can trace how a word evolves semantically over time through
comparison of that word with other similar words. The idea is that the
most similar words are not always fixed in each different year, if a
word acquires a new meaning.</p>
<div id="you-shall-know-a-word-by-the-company-it-keeps---j.-r.-firth-1957" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="you-shall-know-a-word-by-the-company-it-keeps---j.-r.-firth-1957" class="callout-inner">
<h3 class="callout-title">
<strong>You shall know a word by the company
it keeps</strong> - J. R. Firth, 1957</h3>
<div class="callout-content">
<ul><li><p>A word which holds the same meaning across time has stable
contexts and similar words</p></li>
<li><p>A word that instead shifts meaning will be reflected by different
contexts and words</p></li>
</ul><p>So the changes of most similar words reflect the semantic change.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="train-the-word2vec-model">4. Train the Word2Vec model<a class="anchor" aria-label="anchor" href="#train-the-word2vec-model"></a></h2>
<hr class="half-width"><p>Now we will train a two-layer neural network to transform our tokens
into word embeddings. We will be using the library <code>gensim</code>
and the model we will be using is called <code>Word2Vec</code>,
developed by Tomas Mikolov et al. in 2013.</p>
<p>Import the necessary libraries:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a><span class="co"># import logging to monitor training</span></span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a><span class="co"># set up logging</span></span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a>logging.basicConfig(<span class="bu">format</span><span class="op">=</span><span class="st">'</span><span class="sc">%(asctime)s</span><span class="st"> : </span><span class="sc">%(levelname)s</span><span class="st"> : </span><span class="sc">%(message)s</span><span class="st">'</span>, level<span class="op">=</span>logging.INFO)</span></code></pre>
</div>
<p>There are two main architectures for training Word2Vec:</p>
<ul><li>Continuous Bag-of-Words (CBOW): Predicts a target word based on its
surrounding context words.</li>
<li>Continuous Skip-Gram: Predicts surrounding context words given a
target word.</li>
</ul><figure><img src="../fig/emb13.png" class="figure mx-auto d-block"><div class="figcaption">Schematic representations of the different
prediction tasks that CBOW and Skip-gram try to solve</div>
</figure><div id="callout10" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>CBOW is faster to train, while Skip-Gram is more effective for
infrequent words. Increasing context size improves embeddings but
increases training time.</p>
</div>
</div>
</div>
<p>We will be using CBOW. We are interested in having vectors with 300
dimensions and a context size of 5 surrounding words. We include all
words present in the corpora, regardless of their frequency of
occurrence and use 4 CPU cores for training. All these specifics are
translated in only one line of code.</p>
<p>Let’s train our model then:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>model <span class="op">=</span> Word2Vec([tokens_no_stopwords], vector_size<span class="op">=</span><span class="dv">300</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>, sg<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
<p>We can inspect already what’s the output of this training, by
checking the top 5 most similar words to “maan” (moon):</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>word_vectors.most_similar(<span class="st">'maan'</span>, topn<span class="op">=</span><span class="dv">5</span>)</span></code></pre>
</div>
<p><code>[('plek', 0.48467501997947693), ('ouders', 0.46935707330703735), ('supe|', 0.3929591178894043), ('rotterdam', 0.37788015604019165), ('verkeerden', 0.33672046661376953)]</code></p>
<p>We have trained our model on one page only of the newspaper and the
training was very quick. However, to approach our problem it’s best to
train our model on the entire dataset. We dont’ have the resources for
doing that on our local laptop, but luckily for us, <a href="https://zenodo.org/records/3237380" class="external-link">Wevers, M (2019)</a> did that
already for us and released it publicly. Let’s download this dataset on
our laptop and let’s save them in a folder called <code>w2v</code>.</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>folder_path <span class="op">=</span> <span class="st">'data/w2v/'</span></span></code></pre>
</div>
</section><section><h2 class="section-heading" id="load-the-embeddings-and-inspect-them">5. Load the embeddings and inspect them<a class="anchor" aria-label="anchor" href="#load-the-embeddings-and-inspect-them"></a></h2>
<hr class="half-width"><p>We proceed to load our models. We will load all pre-trained model
files from the journal <code>telegraaf</code> into a list. The library
<code>gensim</code> contains a method called <code>KeyedVectors</code>
which allows us to load them.</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> KeyedVectors</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a>filenames_by_decade <span class="op">=</span> [</span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a>    <span class="st">'telegraaf_1950_1959.w2v'</span>,</span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a>    <span class="st">'telegraaf_1960_1969.w2v'</span>,</span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a>    <span class="st">'telegraaf_1970_1979.w2v'</span>,</span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a>    <span class="st">'telegraaf_1980_1989.w2v'</span></span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a>]</span>
<span id="cb31-10"><a href="#cb31-10" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" tabindex="-1"></a><span class="kw">def</span> load_w2v_models(filenames, folder_path):</span>
<span id="cb31-12"><a href="#cb31-12" tabindex="-1"></a>    loaded_models_by_decade <span class="op">=</span> []</span>
<span id="cb31-13"><a href="#cb31-13" tabindex="-1"></a>    </span>
<span id="cb31-14"><a href="#cb31-14" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> filenames:</span>
<span id="cb31-15"><a href="#cb31-15" tabindex="-1"></a>        path <span class="op">=</span> os.path.join(folder_path, <span class="bu">file</span>)</span>
<span id="cb31-16"><a href="#cb31-16" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'loading model </span><span class="sc">{</span>path<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb31-17"><a href="#cb31-17" tabindex="-1"></a>        </span>
<span id="cb31-18"><a href="#cb31-18" tabindex="-1"></a>        model <span class="op">=</span> KeyedVectors.load_word2vec_format(path, binary<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-19"><a href="#cb31-19" tabindex="-1"></a>        </span>
<span id="cb31-20"><a href="#cb31-20" tabindex="-1"></a>        loaded_models_by_decade.append(model)</span>
<span id="cb31-21"><a href="#cb31-21" tabindex="-1"></a>    <span class="cf">return</span> loaded_models_by_decade</span>
<span id="cb31-22"><a href="#cb31-22" tabindex="-1"></a></span>
<span id="cb31-23"><a href="#cb31-23" tabindex="-1"></a><span class="co"># run the function to load the models</span></span>
<span id="cb31-24"><a href="#cb31-24" tabindex="-1"></a>telegraaf_models <span class="op">=</span> load_w2v_models(filenames_by_decade, folder_path)</span></code></pre>
</div>
<p>We should see the following prints:</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a>loading model data<span class="op">/</span>w2v<span class="op">/</span>telegraaf_1950_1959.w2v</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>loading model data<span class="op">/</span>w2v<span class="op">/</span>telegraaf_1960_1969.w2v</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a>loading model data<span class="op">/</span>w2v<span class="op">/</span>telegraaf_1970_1979.w2v</span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>loading model data<span class="op">/</span>w2v<span class="op">/</span>telegraaf_1980_1989.w2v</span></code></pre>
</div>
<p>This means that we have loaded the models correctly.</p>
<p>Now let’s proceed to inspect the top 10 neighbours of the word
<code>mobiel</code> (to start with) across the decades:</p>
<div class="codewrapper sourceCode" id="cb33">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a></span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a>decades <span class="op">=</span> [<span class="st">'50s'</span>, <span class="st">'60s'</span>, <span class="st">'70s'</span>, <span class="st">'80s'</span>]</span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" tabindex="-1"></a><span class="cf">for</span> decade <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(decades)):</span>
<span id="cb33-5"><a href="#cb33-5" tabindex="-1"></a>    top_neighbours <span class="op">=</span> telegraaf_models[decade].most_similar(<span class="st">'mobiel'</span>, topn<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-6"><a href="#cb33-6" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'decade: </span><span class="sc">{</span>decades[decade]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb33-7"><a href="#cb33-7" tabindex="-1"></a>    <span class="cf">for</span> neighbour <span class="kw">in</span> top_neighbours:</span>
<span id="cb33-8"><a href="#cb33-8" tabindex="-1"></a>        <span class="bu">print</span>(neighbour[<span class="dv">0</span>])</span>
<span id="cb33-9"><a href="#cb33-9" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code></pre>
</div>
<p>This is what we should see:</p>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>decade: <span class="dv">50</span><span class="er">s</span></span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a>locatie</span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>stationeren</span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a>landingsvaartuigen</span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a>stationering</span>
<span id="cb34-6"><a href="#cb34-6" tabindex="-1"></a>toegangspoort</span>
<span id="cb34-7"><a href="#cb34-7" tabindex="-1"></a>legeren</span>
<span id="cb34-8"><a href="#cb34-8" tabindex="-1"></a>mustangs</span>
<span id="cb34-9"><a href="#cb34-9" tabindex="-1"></a>imperialistisch</span>
<span id="cb34-10"><a href="#cb34-10" tabindex="-1"></a>landmijnen</span>
<span id="cb34-11"><a href="#cb34-11" tabindex="-1"></a>oprukt</span>
<span id="cb34-12"><a href="#cb34-12" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" tabindex="-1"></a></span>
<span id="cb34-14"><a href="#cb34-14" tabindex="-1"></a>decade: <span class="dv">60</span><span class="er">s</span></span>
<span id="cb34-15"><a href="#cb34-15" tabindex="-1"></a>maagpijnen</span>
<span id="cb34-16"><a href="#cb34-16" tabindex="-1"></a>ureum</span>
<span id="cb34-17"><a href="#cb34-17" tabindex="-1"></a>bagagewagen</span>
<span id="cb34-18"><a href="#cb34-18" tabindex="-1"></a>waterpartij</span>
<span id="cb34-19"><a href="#cb34-19" tabindex="-1"></a>stillere</span>
<span id="cb34-20"><a href="#cb34-20" tabindex="-1"></a>boormachine</span>
<span id="cb34-21"><a href="#cb34-21" tabindex="-1"></a>achterportier</span>
<span id="cb34-22"><a href="#cb34-22" tabindex="-1"></a>doorsnijden</span>
<span id="cb34-23"><a href="#cb34-23" tabindex="-1"></a>stralingsgevaar</span>
<span id="cb34-24"><a href="#cb34-24" tabindex="-1"></a>opgepast</span>
<span id="cb34-25"><a href="#cb34-25" tabindex="-1"></a></span>
<span id="cb34-26"><a href="#cb34-26" tabindex="-1"></a></span>
<span id="cb34-27"><a href="#cb34-27" tabindex="-1"></a>decade: <span class="dv">70</span><span class="er">s</span></span>
<span id="cb34-28"><a href="#cb34-28" tabindex="-1"></a>beweeglijk</span>
<span id="cb34-29"><a href="#cb34-29" tabindex="-1"></a>beweeglijke</span>
<span id="cb34-30"><a href="#cb34-30" tabindex="-1"></a>kunstrubriek</span>
<span id="cb34-31"><a href="#cb34-31" tabindex="-1"></a>kleutert</span>
<span id="cb34-32"><a href="#cb34-32" tabindex="-1"></a>klankbeeld</span>
<span id="cb34-33"><a href="#cb34-33" tabindex="-1"></a>radiojournaal</span>
<span id="cb34-34"><a href="#cb34-34" tabindex="-1"></a>knipperlicht</span>
<span id="cb34-35"><a href="#cb34-35" tabindex="-1"></a>meisjeskoor</span>
<span id="cb34-36"><a href="#cb34-36" tabindex="-1"></a>kinderkoor</span>
<span id="cb34-37"><a href="#cb34-37" tabindex="-1"></a>volksverhalen</span>
<span id="cb34-38"><a href="#cb34-38" tabindex="-1"></a></span>
<span id="cb34-39"><a href="#cb34-39" tabindex="-1"></a></span>
<span id="cb34-40"><a href="#cb34-40" tabindex="-1"></a>decade: <span class="dv">80</span><span class="er">s</span></span>
<span id="cb34-41"><a href="#cb34-41" tabindex="-1"></a>communicatieapparatuur</span>
<span id="cb34-42"><a href="#cb34-42" tabindex="-1"></a>parkeerterreinen</span>
<span id="cb34-43"><a href="#cb34-43" tabindex="-1"></a>sonar</span>
<span id="cb34-44"><a href="#cb34-44" tabindex="-1"></a>alarmsysteem</span>
<span id="cb34-45"><a href="#cb34-45" tabindex="-1"></a>gasinstallaties</span>
<span id="cb34-46"><a href="#cb34-46" tabindex="-1"></a>lichtnet</span>
<span id="cb34-47"><a href="#cb34-47" tabindex="-1"></a>elektromotor</span>
<span id="cb34-48"><a href="#cb34-48" tabindex="-1"></a>inentingen</span>
<span id="cb34-49"><a href="#cb34-49" tabindex="-1"></a>sensoren</span>
<span id="cb34-50"><a href="#cb34-50" tabindex="-1"></a>hulpbehoevenden</span></code></pre>
</div>
<p>Let’s inspect together these results:</p>
<p>In the 50s, the neighbouring words predominantly point towards
military and geopolitical terms (stationeren, landingsvaartuigen,
stationering, legeren, and landmijnen). The presence of the word
<em>imperialistisch</em> also suggests discussions about imperialism,
possibly reflecting post WWII tensions (in the 50s Europe was entering
Cold War period).</p>
<p>In the 60s, the term is associated to meanings related to health,
safety and mechanical terms. <em>Stralingsgevaar</em> and
<em>opgepast</em> suggest concerns about radiation and the need for
caution, possibly reflecting the nuclear anxieties of the era.</p>
<p>In the 70s the word is associated to technological advancement and
culture. While finally in the 80s we see a list of words that have solid
grounds in technological and infrastracture terms. Words like
<em>communicatieapparatuur</em>, <em>sonar</em>, <em>alarmsysteem</em>,
<em>elektromotor</em>, and <em>sensoren</em> signals the push that
technology has had in this period, with the advent of mobile phones
(communicatieapparatuur).</p>
<p>All in all, the word’s meaning evolved from being a means of
transport to a modern technology tool employed in urban infrastructure,
societal well-being and communication.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Reproduce the steps above for the other words: <code>televisie</code>
and <code>ijzeren</code>. What do you expect from their historical
semantic evolution? Television was already present in the 50s, although
the technology around it has evolved up to the 1989. And what about the
term <code>iron</code>? When do you expect this term to acquire a
meaning related to the Cold War e.g. <a href="https://en.wikipedia.org/wiki/Iron_Curtain" class="external-link">Iron Curtain</a>?</p>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/00-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/02-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/00-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction
        </a>
        <a class="chapter-link float-end" href="../instructor/02-transformers.html" rel="next">
          Next: Episode 2: BERT and...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-preprocessing.Rmd" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.10" class="external-link">sandpaper (0.16.10)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.7" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/01-preprocessing.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "Episode 1: From text to vectors",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/01-preprocessing.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/01-preprocessing.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2024-12-31",
  "datePublished": "2024-12-31"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

